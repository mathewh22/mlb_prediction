{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLB Predictor Project\n",
    "\n",
    "Group 21, Plotters for Success\n",
    "\n",
    "Gerardo Skrut, Victor Gikunda, Mathew Huang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to importing the data, we cleaned and explored the existing data.\n",
    "\n",
    "Finally, After we consolidated two datasets with pitching and batting, we are going to separate each portion to inputs and outputs respectively. \n",
    "\n",
    "Our inputs overall would include Left Field, Right Field, and Centerfield Distance, Maximum and minimum wall height, Day/night, Attendance, Precipitation, Sky Condition, Temperature, Wind Direction, and Wind Speed. \n",
    "\n",
    "For Pitching specifically, we will be using the pitcher's **Season ERA** from the 2023 Season. \n",
    "\n",
    "For Batting Specifically, we will be using the batter's **Season Batting Average** from the 2023 Season.\n",
    "\n",
    "Our outputs would be game specific statistics. \n",
    "\n",
    "For Pitching, we would have the number of Hits Allowed, Runs Allowed, Earned Runs, Walks Given, Hit by Pitches, and Wild Pitches.\n",
    "\n",
    "For Batting, we would have the number of Hits, Doubles, Triples, Home Runs, RBIs, Walks, and Strikeouts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ballpark Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Column Values:\n",
      "left_field         331.833333\n",
      "center_field       404.166667\n",
      "right_field        328.333333\n",
      "min_wall_height      7.553333\n",
      "max_wall_height     14.266667\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load ballparks data\n",
    "data = pd.read_csv('ballparks.csv')\n",
    "\n",
    "# Filter for relevant columns\n",
    "columns_to_keep = ['team_name', 'ballpark', 'left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height']\n",
    "data_filtered = data[columns_to_keep]\n",
    "\n",
    "# Calculate average for numeric columns\n",
    "average_values = data_filtered[['left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height']].mean()\n",
    "\n",
    "# Print the average values\n",
    "print(\"Average Column Values:\")\n",
    "print(average_values)\n",
    "\n",
    "# Save the filtered data\n",
    "data_filtered.to_csv('2023_filtered_ballpark_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Information Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Values for Numeric Columns:\n",
      "attendance    29356.347087\n",
      "temp             72.413835\n",
      "windspeed         6.466828\n",
      "dtype: float64\n",
      "\n",
      "Most Frequent Values for Categorical Columns:\n",
      "daynight      night\n",
      "precip         none\n",
      "sky          cloudy\n",
      "winddir     unknown\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load game info data\n",
    "data = pd.read_csv('2023gameinfo.csv')\n",
    "\n",
    "# Filter for relevant columns\n",
    "columns_to_keep = ['gid', 'daynight', 'attendance', 'precip', 'sky', 'temp', 'winddir', 'windspeed']\n",
    "data_filtered = data[columns_to_keep]\n",
    "\n",
    "# Calculate average for numeric columns\n",
    "numeric_columns = ['attendance', 'temp', 'windspeed']\n",
    "average_values = data_filtered[numeric_columns].mean()\n",
    "\n",
    "# Find the most frequent value for categorical columns\n",
    "categorical_columns = ['daynight', 'precip', 'sky', 'winddir']\n",
    "most_frequent_values = data_filtered[categorical_columns].mode().iloc[0]\n",
    "\n",
    "# Print results\n",
    "print(\"Average Values for Numeric Columns:\")\n",
    "print(average_values)\n",
    "print(\"\\nMost Frequent Values for Categorical Columns:\")\n",
    "print(most_frequent_values)\n",
    "\n",
    "# Save filtered data\n",
    "data_filtered.to_csv('2023_filtered_gameinfo_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batting Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''filters for relevant batting player data'''\n",
    "# load csv file\n",
    "data = pd.read_csv('2023batting.csv')\n",
    "\n",
    "# filters for columns with relevant data\n",
    "columns_to_keep = ['gid', 'id', 'team', 'b_ab', 'b_h', 'b_d', 'b_t', 'b_hr', 'b_rbi', 'b_w', 'b_k', 'date', 'wl']  \n",
    "\n",
    "# creates a new data frame\n",
    "data_filtered = data[columns_to_keep]\n",
    "\n",
    "# save new csv file\n",
    "data_filtered.to_csv('2023_filtered_batting_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dataset includes batting data for players who do not bat, this filters out those players who had zero at bats (i.e. pitchers)'''\n",
    "\n",
    "updated_rows = []\n",
    "with open('2023_filtered_batting_data.csv', 'r') as data_file:  \n",
    "    data_reader = csv.reader(data_file)\n",
    "    header = next(data_reader)  \n",
    "    updated_rows.append(header)\n",
    "\n",
    "    # reads each row\n",
    "    for row in data_reader:\n",
    "        # checks if players number of plate appearances is zero\n",
    "        if int(row[3]) == 0:\n",
    "            # skips if plate appearance is equal to zero  \n",
    "            continue  \n",
    "\n",
    "        # appends data if not    \n",
    "        updated_rows.append(row)\n",
    "\n",
    "# creates a new csv file with updated data\n",
    "with open('2023_batting_data_cleaned.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.writer(updated_file)\n",
    "    writer.writerows(updated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Merges relevant data into a one csv file'''\n",
    "\n",
    "# loads ball park data into a dictionary\n",
    "stadium_data = {}\n",
    "with open('2023_filtered_ballpark_data.csv', 'r') as stadium_file:  \n",
    "    stadium_reader = csv.DictReader(stadium_file)\n",
    "    for row in stadium_reader:\n",
    "        # uses the team name as the key for each entry\n",
    "        team_name = row['team_name']  \n",
    "        # stores the data for the corresponding team\n",
    "        stadium_data[team_name] = row  \n",
    "\n",
    "# loads game info data into a dictionary       \n",
    "gameinfo_data = {}\n",
    "with open('2023_filtered_gameinfo_data.csv', 'r') as gameinfo_file:  \n",
    "    gameinfo_reader = csv.DictReader(gameinfo_file)\n",
    "    for row in gameinfo_reader:\n",
    "        # uses the game id as the key for each entry\n",
    "        gid = row['gid']  \n",
    "        # stores the data for the corresponding game id \n",
    "        gameinfo_data[gid] = row  \n",
    "        \n",
    "\n",
    "# read batting data, merge with ball park data, store updated rows\n",
    "updated_rows = []\n",
    "with open('2023_batting_data_cleaned.csv', 'r') as game_log_file: \n",
    "    batting_log = csv.reader(game_log_file)\n",
    "    # captures the header row\n",
    "    header = next(batting_log)\n",
    "\n",
    "    # extracts column names of the ball park data\n",
    "    stadium_columns = list(stadium_data.values())[0].keys() \n",
    "    # extracts column names of the game info data, excluding 'gid' since it already exist in the dataset \n",
    "    gameinfo_columns = list(gameinfo_data.values())[0].keys()\n",
    "    new_gameinfo_columns = []\n",
    "    \n",
    "    for col in gameinfo_columns:\n",
    "        if col != 'gid':\n",
    "            new_gameinfo_columns.append(col)\n",
    "\n",
    "    # appens the original header with additional stadium and game info columns           \n",
    "    updated_rows.append(header + list(stadium_columns) + new_gameinfo_columns)          \n",
    "\n",
    "    # iterates through each row of the batting log data      \n",
    "    for row in batting_log:\n",
    "        \n",
    "        # extracts game id\n",
    "        gid = row[0] \n",
    "        # extracts first three letters of the game id, which is the team id\n",
    "        team_id = gid[:3]\n",
    "\n",
    "        # checks if the team id exists in the stadium data        \n",
    "        if team_id in stadium_data:\n",
    "            # retieves ball park data for that team and appends it to the row\n",
    "            # this can be done because the first three characters of the game id represent the team id of the home team \n",
    "            stadium_info = stadium_data[team_id]\n",
    "            row.extend(stadium_info.values())\n",
    "        \n",
    "        # checks if game id is in the game info data\n",
    "        if gid in gameinfo_data:\n",
    "            \n",
    "            # retrieves game info data to corresponding game id\n",
    "            gameinfo = gameinfo_data[gid]\n",
    "            updated_gameinfo = []\n",
    "            # removes the duplicate game id column and appends the other values\n",
    "            for key, value in gameinfo.items():\n",
    "                if key != 'gid':\n",
    "                    updated_gameinfo.append(value)\n",
    "            \n",
    "            # adds game info data to current row\n",
    "            row.extend(updated_gameinfo)\n",
    "\n",
    "        # adds new data to row   \n",
    "        updated_rows.append(row)\n",
    "\n",
    "\n",
    "# new csv file output with updated data\n",
    "with open('2023_merged_batting_data.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.writer(updated_file)\n",
    "    writer.writerows(updated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Batting Average: 0.226\n"
     ]
    }
   ],
   "source": [
    "'''adds a row with calculated season average for each player since the data set only includes specific game log data'''\n",
    " \n",
    "# Dictionary to store season totals\n",
    "season_avg_data = {}\n",
    "\n",
    "# Process input file to calculate season totals for each player\n",
    "with open('2023_merged_batting_data.csv', 'r') as stat_file:\n",
    "    stat_reader = csv.DictReader(stat_file)\n",
    "    header = stat_reader.fieldnames + ['season_batting_avg']\n",
    "\n",
    "    for row in stat_reader:\n",
    "        name = row['id']\n",
    "        bats = int(row['b_ab'])\n",
    "        hits = int(row['b_h'])\n",
    "\n",
    "        # Update season totals for the player\n",
    "        if name in season_avg_data:\n",
    "            season_avg_data[name]['total_at_bats'] += bats\n",
    "            season_avg_data[name]['total_hits'] += hits\n",
    "        else:\n",
    "            season_avg_data[name] = {'total_at_bats': bats, 'total_hits': hits}\n",
    "\n",
    "# Prepare updated rows with calculated season batting averages\n",
    "updated_rows = []\n",
    "with open('2023_merged_batting_data.csv', 'r') as stat_file:\n",
    "    stat_reader = csv.DictReader(stat_file)\n",
    "    for row in stat_reader:\n",
    "        name = row['id']\n",
    "        total_bats = season_avg_data[name]['total_at_bats']\n",
    "        total_hits = season_avg_data[name]['total_hits']\n",
    "\n",
    "        # Calculate player's season batting average\n",
    "        batting_average = total_hits / total_bats\n",
    "        row['season_batting_avg'] = f\"{batting_average:.3f}\"\n",
    "\n",
    "        updated_rows.append(row)\n",
    "\n",
    "# Write updated data to a new CSV file\n",
    "with open('2023_complete_batting_data.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.DictWriter(updated_file, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(updated_rows)\n",
    "\n",
    "# Calculate overall average batting average (NEW ADDITION)\n",
    "batting_averages = [\n",
    "    stats['total_hits'] / stats['total_at_bats']\n",
    "    for stats in season_avg_data.values()\n",
    "    if stats['total_at_bats'] > 0  # Avoid division by zero\n",
    "]\n",
    "\n",
    "average_batting_avg = sum(batting_averages) / len(batting_averages) if batting_averages else 0\n",
    "\n",
    "# Print the overall average batting average (does not affect functionality)\n",
    "print(f\"Average Batting Average: {average_batting_avg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''formats finalzied dataset properly'''\n",
    "\n",
    "# creates a mapping of player IDs to their full names\n",
    "name_id_map = {}\n",
    "with open('2023allplayers.csv', 'r') as name_file:  \n",
    "    name_reader = csv.DictReader(name_file)\n",
    "    for row in name_reader:\n",
    "        # combines first and last name columns to form the full name and maps it respectively \n",
    "        name_id_map[row['id']] = f\"{row['first']} {row['last']}\"\n",
    "\n",
    "# reads the baseball data file and replace IDs with full names\n",
    "updated_rows = []\n",
    "with open('2023_complete_batting_data.csv', 'r') as data_file: \n",
    "    data_reader = csv.reader(data_file)\n",
    "    header = next(data_reader)  \n",
    "    updated_rows.append(header)\n",
    "\n",
    "    # iterates through each row of the data\n",
    "    for row in data_reader:\n",
    "        \n",
    "        # converts the date format from 'YYYYMMDD' to 'MM/DD/YYYY'\n",
    "        date_str = row[11]\n",
    "        date_format = datetime.strptime(date_str, '%Y%m%d').strftime('%m/%d/%Y')\n",
    "        row[11] = date_format\n",
    "        \n",
    "        # replaces the player id with the player's full name\n",
    "        player_id = row[1] \n",
    "        if player_id in name_id_map:\n",
    "            row[1] = name_id_map[player_id] \n",
    "\n",
    "        # adds the updated row\n",
    "        updated_rows.append(row)\n",
    "\n",
    "# writes the updated csv file with all data and cleaned\n",
    "with open('2023_full_batting_stats_cleaned.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.writer(updated_file)\n",
    "    writer.writerows(updated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            gid                 id team  b_ab  b_h  b_d  b_t  b_hr  b_rbi  \\\n",
      "0  BOS202303300     Cedric Mullins  BAL     4    1    0    0     0      1   \n",
      "1  BOS202303300    Adley Rutschman  BAL     5    5    0    0     1      4   \n",
      "2  BOS202303300  Anthony Santander  BAL     6    2    1    0     0      0   \n",
      "3  BOS202303300   Ryan Mountcastle  BAL     4    1    1    0     0      1   \n",
      "4  BOS202303300   Gunnar Henderson  BAL     3    0    0    0     0      0   \n",
      "\n",
      "   b_w  b_k        date wl team_name     ballpark left_field center_field  \\\n",
      "0    2    1  03/30/2023  w       BOS  Fenway Park        310          420   \n",
      "1    1    0  03/30/2023  w       BOS  Fenway Park        310          420   \n",
      "2    0    2  03/30/2023  w       BOS  Fenway Park        310          420   \n",
      "3    2    0  03/30/2023  w       BOS  Fenway Park        310          420   \n",
      "4    2    2  03/30/2023  w       BOS  Fenway Park        310          420   \n",
      "\n",
      "   right_field min_wall_height  max_wall_height daynight  attendance precip  \\\n",
      "0          302             3.0               37      day     36049.0   none   \n",
      "1          302             3.0               37      day     36049.0   none   \n",
      "2          302             3.0               37      day     36049.0   none   \n",
      "3          302             3.0               37      day     36049.0   none   \n",
      "4          302             3.0               37      day     36049.0   none   \n",
      "\n",
      "     sky  temp winddir  windspeed  season_batting_avg  \n",
      "0  sunny  38.0    ltor       12.0               0.226  \n",
      "1  sunny  38.0    ltor       12.0               0.273  \n",
      "2  sunny  38.0    ltor       12.0               0.257  \n",
      "3  sunny  38.0    ltor       12.0               0.267  \n",
      "4  sunny  38.0    ltor       12.0               0.260  \n"
     ]
    }
   ],
   "source": [
    "final_csv = '2023_full_batting_stats_cleaned.csv'\n",
    "\n",
    "data = pd.read_csv(final_csv, low_memory = False)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitching Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''filters for relevant player pitching data'''\n",
    "# load csv file\n",
    "data = pd.read_csv('2023pitching.csv')\n",
    "\n",
    "# filters for columns with relevant data\n",
    "columns_to_keep = ['gid', 'id', 'team', 'p_ipouts', 'p_seq', 'p_h', 'p_r', 'p_er', 'p_w','p_k', 'p_hbp', 'p_wp', 'date', 'wl']  \n",
    "\n",
    "# creates a new data frame\n",
    "data_filtered = data[columns_to_keep]\n",
    "\n",
    "# save new csv file\n",
    "data_filtered.to_csv('2023_filtered_pitching_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''merges pitcher game log data with ball park and game info data'''\n",
    "\n",
    "# load ball park data into a dictionary\n",
    "stadium_data = {}\n",
    "with open('2023_filtered_ballpark_data.csv', 'r') as stadium_file:  \n",
    "    stadium_reader = csv.DictReader(stadium_file)\n",
    "    for row in stadium_reader:\n",
    "        # extracts the team name column\n",
    "        team_name = row['team_name'] \n",
    "        # uses team name as a key and the row as the value \n",
    "        stadium_data[team_name] = row  \n",
    "\n",
    "# loads game info data into a dictionary        \n",
    "gameinfo_data = {}\n",
    "with open('2023_filtered_gameinfo_data.csv', 'r') as gameinfo_file:  \n",
    "    gameinfo_reader = csv.DictReader(gameinfo_file)\n",
    "    for row in gameinfo_reader:\n",
    "        # extracts the 'gid' column\n",
    "        gid = row['gid']  \n",
    "        # uses the game id as a key and the row as the value\n",
    "        gameinfo_data[gid] = row  \n",
    "        \n",
    "\n",
    "# read pitching data, merge with ball park data, store updated rows\n",
    "updated_rows = []\n",
    "with open('2023_filtered_pitching_data.csv', 'r') as game_log_file: \n",
    "    pitching_log = csv.reader(game_log_file)\n",
    "    # captures the header row\n",
    "    header = next(pitching_log)  \n",
    "\n",
    "\n",
    "    # extarcts column names of ball park data\n",
    "    stadium_columns = list(stadium_data.values())[0].keys()  \n",
    "    # extracts column names of the game info data, excluding 'gid' since it already exist in the dataset\n",
    "    gameinfo_columns = list(gameinfo_data.values())[0].keys()\n",
    "    new_gameinfo_columns = []\n",
    "    \n",
    "    for col in gameinfo_columns:\n",
    "        if col != 'gid':\n",
    "            new_gameinfo_columns.append(col)\n",
    "\n",
    "    # appends the original header with additional stadium and game info columns           \n",
    "    updated_rows.append(header + list(stadium_columns) + new_gameinfo_columns)\n",
    "\n",
    "\n",
    "    # interates through each row of the pitching log data\n",
    "    for row in pitching_log:\n",
    "        # extracts the game id\n",
    "        gid = row[0]  \n",
    "        # extracts the first three letter of the game id, which is the team id\n",
    "        team_id = gid[:3] \n",
    "\n",
    "        # checks if the team id exists in the staidum data \n",
    "        if team_id in stadium_data:\n",
    "            \n",
    "            # retrieves he ball park data for that team and appends it to the row\n",
    "            # this can be done because the first three character of the game id represent the team if of the home team\n",
    "            stadium_info = stadium_data[team_id]\n",
    "            row.extend(stadium_info.values())\n",
    "        \n",
    "        # checks if the game id is in the game info data \n",
    "        if gid in gameinfo_data:\n",
    "            \n",
    "            # retrieves game info data to corresponding game id \n",
    "            gameinfo = gameinfo_data[gid]\n",
    "            updated_gameinfo = []\n",
    "            # removes the duplicate game id column and appends the other values\n",
    "            for key, value in gameinfo.items():\n",
    "                if key != 'gid':\n",
    "                    updated_gameinfo.append(value)\n",
    "\n",
    "            # adds game info data to current row\n",
    "            row.extend(updated_gameinfo)\n",
    "\n",
    "        # adds new data to row   \n",
    "        updated_rows.append(row)\n",
    "\n",
    "\n",
    "# new csv file output with updated data\n",
    "with open('2023_merged_pitching_data.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.writer(updated_file)\n",
    "    writer.writerows(updated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Season ERA: 5.870\n"
     ]
    }
   ],
   "source": [
    "'''calculates the season ERA for each pitcher and merge the dataset'''\n",
    "import csv\n",
    "\n",
    "# Initialize dictionary to store season totals\n",
    "season_avg_data = {}\n",
    "\n",
    "# Process input file to calculate season ERA totals for each pitcher\n",
    "with open('2023_merged_pitching_data.csv', 'r') as stat_file:\n",
    "    stat_reader = csv.DictReader(stat_file)\n",
    "    header = stat_reader.fieldnames + ['season_era']\n",
    "\n",
    "    for row in stat_reader:\n",
    "        name = row['id']\n",
    "        outs = int(row['p_ipouts'])\n",
    "        earned_runs = int(row['p_er'])\n",
    "\n",
    "        # Update season totals for the pitcher\n",
    "        if name in season_avg_data:\n",
    "            season_avg_data[name]['total_innings'] += (outs / 3)\n",
    "            season_avg_data[name]['total_earned_runs'] += earned_runs\n",
    "        else:\n",
    "            season_avg_data[name] = {'total_innings': (outs / 3), 'total_earned_runs': earned_runs}\n",
    "\n",
    "# Prepare updated rows with calculated season ERA\n",
    "updated_rows = []\n",
    "with open('2023_merged_pitching_data.csv', 'r') as stat_file:\n",
    "    stat_reader = csv.DictReader(stat_file)\n",
    "    for row in stat_reader:\n",
    "        name = row['id']\n",
    "        innings = season_avg_data[name]['total_innings']\n",
    "        earned_runs = season_avg_data[name]['total_earned_runs']\n",
    "\n",
    "        # Calculate season ERA\n",
    "        season_era = 9 * (earned_runs / innings)\n",
    "        row['season_era'] = f\"{season_era:.3f}\"\n",
    "\n",
    "        updated_rows.append(row)\n",
    "\n",
    "# Write updated data to a new CSV file\n",
    "with open('2023_complete_pitching_data.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.DictWriter(updated_file, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(updated_rows)\n",
    "\n",
    "# Calculate overall average season ERA (NEW ADDITION)\n",
    "season_eras = [\n",
    "    9 * (stats['total_earned_runs'] / stats['total_innings'])\n",
    "    for stats in season_avg_data.values()\n",
    "    if stats['total_innings'] > 0  # Avoid division by zero\n",
    "]\n",
    "\n",
    "average_season_era = sum(season_eras) / len(season_eras) if season_eras else 0\n",
    "\n",
    "# Print the overall average season ERA (does not affect functionality)\n",
    "print(f\"Average Season ERA: {average_season_era:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: ['gid', 'id', 'team', 'p_ipouts', 'p_seq', 'p_h', 'p_r', 'p_er', 'p_w', 'p_k', 'p_hbp', 'p_wp', 'date', 'wl', 'team_name', 'ballpark', 'left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height', 'daynight', 'attendance', 'precip', 'sky', 'temp', 'winddir', 'windspeed', 'season_era']\n",
      "Date column detected at index 12 based on column name.\n",
      "Data cleaning complete. Updated dataset saved.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the first row (header) to see the column names\n",
    "with open('2023_complete_pitching_data.csv', 'r') as data_file:\n",
    "    data_reader = csv.reader(data_file)\n",
    "    header = next(data_reader)\n",
    "    print(\"Columns in the dataset:\", header)\n",
    "\n",
    "# Automatically detect date column by checking column names or data format\n",
    "date_column_index = None\n",
    "\n",
    "# Detect column with YYYYMMDD pattern or name containing \"date\"\n",
    "for i, column in enumerate(header):\n",
    "    if \"date\" in column.lower():  # Name-based detection\n",
    "        date_column_index = i\n",
    "        print(f\"Date column detected at index {date_column_index} based on column name.\")\n",
    "        break\n",
    "\n",
    "if date_column_index is None:\n",
    "    # Fallback to detecting by data format in the first row\n",
    "    for i, row in enumerate(data_reader):\n",
    "        for col_index, value in enumerate(row):\n",
    "            try:\n",
    "                if len(value) == 8:  # Typical YYYYMMDD format length\n",
    "                    datetime.strptime(value, '%Y%m%d')\n",
    "                    date_column_index = col_index\n",
    "                    print(f\"Date column detected at index {date_column_index} based on data format.\")\n",
    "                    break\n",
    "            except ValueError:\n",
    "                continue\n",
    "        if date_column_index is not None:\n",
    "            break\n",
    "\n",
    "# Update rows with automatic date detection\n",
    "name_id_map = {}\n",
    "with open('2023allplayers.csv', 'r') as name_file:  \n",
    "    name_reader = csv.DictReader(name_file)\n",
    "    for row in name_reader:\n",
    "        name_id_map[row['id']] = f\"{row['first']} {row['last']}\"\n",
    "\n",
    "updated_rows = []\n",
    "\n",
    "with open('2023_complete_pitching_data.csv', 'r') as data_file: \n",
    "    data_reader = csv.reader(data_file)\n",
    "    header = next(data_reader)\n",
    "    updated_rows.append(header)\n",
    "\n",
    "    for row in data_reader:\n",
    "        if date_column_index is not None:\n",
    "            # Convert detected date column format\n",
    "            try:\n",
    "                date_str = row[date_column_index]\n",
    "                date_format = datetime.strptime(date_str, '%Y%m%d').strftime('%m/%d/%Y')\n",
    "                row[date_column_index] = date_format\n",
    "            except ValueError:\n",
    "                pass  # Handle rows where the date might not conform\n",
    "\n",
    "        # Replace player ID with name\n",
    "        player_id = row[1]\n",
    "        if player_id in name_id_map:\n",
    "            row[1] = name_id_map[player_id]\n",
    "\n",
    "        updated_rows.append(row)\n",
    "\n",
    "# Write cleaned data\n",
    "with open('2023_full_pitching_stats_cleaned.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.writer(updated_file)\n",
    "    writer.writerows(updated_rows)\n",
    "\n",
    "print(\"Data cleaning complete. Updated dataset saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            gid               id team  p_ipouts  p_seq  p_h  p_r  p_er  p_w  \\\n",
      "0  BOS202303300      Kyle Gibson  BAL        15      1    6    4     4    1   \n",
      "1  BOS202303300      Keegan Akin  BAL         3      2    1    0     0    0   \n",
      "2  BOS202303300     Cionel Perez  BAL         3      3    0    0     0    0   \n",
      "3  BOS202303300      Bryan Baker  BAL         2      4    2    3     3    1   \n",
      "4  BOS202303300  Logan Gillaspie  BAL         1      5    0    0     0    0   \n",
      "\n",
      "   p_k  p_hbp  p_wp        date wl team_name     ballpark left_field  \\\n",
      "0    3      1     0  03/30/2023  w       BOS  Fenway Park        310   \n",
      "1    2      0     0  03/30/2023  w       BOS  Fenway Park        310   \n",
      "2    0      0     0  03/30/2023  w       BOS  Fenway Park        310   \n",
      "3    1      1     0  03/30/2023  w       BOS  Fenway Park        310   \n",
      "4    1      0     0  03/30/2023  w       BOS  Fenway Park        310   \n",
      "\n",
      "  center_field  right_field min_wall_height  max_wall_height daynight  \\\n",
      "0          420          302             3.0               37      day   \n",
      "1          420          302             3.0               37      day   \n",
      "2          420          302             3.0               37      day   \n",
      "3          420          302             3.0               37      day   \n",
      "4          420          302             3.0               37      day   \n",
      "\n",
      "   attendance precip    sky  temp winddir  windspeed  season_era  \n",
      "0     36049.0   none  sunny  38.0    ltor       12.0       4.708  \n",
      "1     36049.0   none  sunny  38.0    ltor       12.0       6.845  \n",
      "2     36049.0   none  sunny  38.0    ltor       12.0       3.436  \n",
      "3     36049.0   none  sunny  38.0    ltor       12.0       4.169  \n",
      "4     36049.0   none  sunny  38.0    ltor       12.0       6.000  \n"
     ]
    }
   ],
   "source": [
    "final_csv = '2023_full_pitching_stats_cleaned.csv'\n",
    "\n",
    "data = pd.read_csv(final_csv)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-input processing\n",
    "\n",
    "Prior to using the cleaned data, we need to process the data into a more readable format. This would mean fully separating into inputs and outputs as well as converting any categorical variables into binaries. \n",
    "\n",
    "To do so, we use pd.get_dummies to \"one-hot-encode\" our categorical variables to not place too much importance on any given point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21042 entries, 0 to 21061\n",
      "Data columns (total 28 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   left_field       21042 non-null  float64\n",
      " 1   center_field     21042 non-null  float64\n",
      " 2   right_field      21042 non-null  float64\n",
      " 3   min_wall_height  21042 non-null  float64\n",
      " 4   max_wall_height  21042 non-null  float64\n",
      " 5   attendance       21042 non-null  float64\n",
      " 6   temp             21042 non-null  float64\n",
      " 7   windspeed        21042 non-null  float64\n",
      " 8   season_era       21042 non-null  float64\n",
      " 9   daynight_day     21042 non-null  int32  \n",
      " 10  daynight_night   21042 non-null  int32  \n",
      " 11  precip_drizzle   21042 non-null  int32  \n",
      " 12  precip_none      21042 non-null  int32  \n",
      " 13  precip_rain      21042 non-null  int32  \n",
      " 14  precip_snow      21042 non-null  int32  \n",
      " 15  sky_cloudy       21042 non-null  int32  \n",
      " 16  sky_dome         21042 non-null  int32  \n",
      " 17  sky_overcast     21042 non-null  int32  \n",
      " 18  sky_sunny        21042 non-null  int32  \n",
      " 19  winddir_fromcf   21042 non-null  int32  \n",
      " 20  winddir_fromlf   21042 non-null  int32  \n",
      " 21  winddir_fromrf   21042 non-null  int32  \n",
      " 22  winddir_ltor     21042 non-null  int32  \n",
      " 23  winddir_rtol     21042 non-null  int32  \n",
      " 24  winddir_tocf     21042 non-null  int32  \n",
      " 25  winddir_tolf     21042 non-null  int32  \n",
      " 26  winddir_torf     21042 non-null  int32  \n",
      " 27  winddir_unknown  21042 non-null  int32  \n",
      "dtypes: float64(9), int32(19)\n",
      "memory usage: 3.1 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21042 entries, 0 to 21061\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   p_ipouts  21042 non-null  int64\n",
      " 1   p_h       21042 non-null  int64\n",
      " 2   p_r       21042 non-null  int64\n",
      " 3   p_er      21042 non-null  int64\n",
      " 4   p_w       21042 non-null  int64\n",
      " 5   p_k       21042 non-null  int64\n",
      " 6   p_hbp     21042 non-null  int64\n",
      " 7   p_wp      21042 non-null  int64\n",
      "dtypes: int64(8)\n",
      "memory usage: 1.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load pitching data\n",
    "pitching_data = pd.read_csv('2023_full_pitching_stats_cleaned.csv')\n",
    "\n",
    "# Define categorical data and input/output columns\n",
    "categorical_data = ['daynight', 'precip', 'sky', 'winddir']\n",
    "\n",
    "pitching_inputs = [\n",
    "    'left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height',\n",
    "    'attendance', 'temp', 'windspeed', 'season_era', 'daynight_day', 'daynight_night',\n",
    "    'precip_drizzle', 'precip_none', 'precip_rain', 'precip_snow', 'sky_cloudy', 'sky_dome',\n",
    "    'sky_overcast', 'sky_sunny', 'winddir_fromcf', 'winddir_fromlf', 'winddir_fromrf',\n",
    "    'winddir_ltor', 'winddir_rtol', 'winddir_tocf', 'winddir_tolf', 'winddir_torf', 'winddir_unknown'\n",
    "]\n",
    "pitching_outputs = ['p_ipouts', 'p_h', 'p_r', 'p_er', 'p_w', 'p_k','p_hbp', 'p_wp']\n",
    "\n",
    "# Handle missing values and categorical data\n",
    "pitching_data['precip'] = pitching_data['precip'].fillna('none')\n",
    "pitching_data = pitching_data.dropna()\n",
    "pitching_data = pd.get_dummies(pitching_data, columns=categorical_data)\n",
    "\n",
    "# Post-encoding dummy variables\n",
    "p_encoded_variables = [\n",
    "    'daynight_day', 'daynight_night', 'precip_drizzle', 'precip_none', 'precip_rain',\n",
    "    'precip_snow', 'sky_cloudy', 'sky_dome', 'sky_overcast', 'sky_sunny',\n",
    "    'winddir_fromcf', 'winddir_fromlf', 'winddir_fromrf', 'winddir_ltor', 'winddir_rtol',\n",
    "    'winddir_tocf', 'winddir_tolf', 'winddir_torf', 'winddir_unknown'\n",
    "]\n",
    "\n",
    "# Convert non-dummy columns to float\n",
    "non_encoded_columns = [\n",
    "    'left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height',\n",
    "    'attendance', 'temp', 'windspeed', 'season_era'\n",
    "]\n",
    "pitching_data[non_encoded_columns] = pitching_data[non_encoded_columns].astype(float)\n",
    "\n",
    "# Convert dummy-encoded columns to float\n",
    "pitching_data[p_encoded_variables] = pitching_data[p_encoded_variables].astype(int)\n",
    "\n",
    "# Extract input and output data\n",
    "pitching_input_data = pitching_data[pitching_inputs]\n",
    "pitching_output_data = pitching_data[pitching_outputs]\n",
    "\n",
    "# Verify data types\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(pitching_input_data.info())\n",
    "print(pitching_output_data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_ipouts</th>\n",
       "      <th>p_h</th>\n",
       "      <th>p_r</th>\n",
       "      <th>p_er</th>\n",
       "      <th>p_w</th>\n",
       "      <th>p_k</th>\n",
       "      <th>p_hbp</th>\n",
       "      <th>p_wp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_ipouts  p_h  p_r  p_er  p_w  p_k  p_hbp  p_wp\n",
       "0        15    6    4     4    1    3      1     0\n",
       "1         3    1    0     0    0    2      0     0\n",
       "2         3    0    0     0    0    0      0     0\n",
       "3         2    2    3     3    1    1      1     0\n",
       "4         1    0    0     0    0    1      0     0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitching_input_data.head()\n",
    "pitching_output_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49080 entries, 0 to 49118\n",
      "Data columns (total 28 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   left_field          49080 non-null  float64\n",
      " 1   center_field        49080 non-null  float64\n",
      " 2   right_field         49080 non-null  float64\n",
      " 3   min_wall_height     49080 non-null  float64\n",
      " 4   max_wall_height     49080 non-null  float64\n",
      " 5   attendance          49080 non-null  float64\n",
      " 6   temp                49080 non-null  float64\n",
      " 7   windspeed           49080 non-null  float64\n",
      " 8   season_batting_avg  49080 non-null  float64\n",
      " 9   daynight_day        49080 non-null  int32  \n",
      " 10  daynight_night      49080 non-null  int32  \n",
      " 11  precip_drizzle      49080 non-null  int32  \n",
      " 12  precip_none         49080 non-null  int32  \n",
      " 13  precip_rain         49080 non-null  int32  \n",
      " 14  precip_snow         49080 non-null  int32  \n",
      " 15  sky_cloudy          49080 non-null  int32  \n",
      " 16  sky_dome            49080 non-null  int32  \n",
      " 17  sky_overcast        49080 non-null  int32  \n",
      " 18  sky_sunny           49080 non-null  int32  \n",
      " 19  winddir_fromcf      49080 non-null  int32  \n",
      " 20  winddir_fromlf      49080 non-null  int32  \n",
      " 21  winddir_fromrf      49080 non-null  int32  \n",
      " 22  winddir_ltor        49080 non-null  int32  \n",
      " 23  winddir_rtol        49080 non-null  int32  \n",
      " 24  winddir_tocf        49080 non-null  int32  \n",
      " 25  winddir_tolf        49080 non-null  int32  \n",
      " 26  winddir_torf        49080 non-null  int32  \n",
      " 27  winddir_unknown     49080 non-null  int32  \n",
      "dtypes: float64(9), int32(19)\n",
      "memory usage: 7.3 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49080 entries, 0 to 49118\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   b_ab    49080 non-null  int64\n",
      " 1   b_h     49080 non-null  int64\n",
      " 2   b_d     49080 non-null  int64\n",
      " 3   b_t     49080 non-null  int64\n",
      " 4   b_hr    49080 non-null  int64\n",
      " 5   b_rbi   49080 non-null  int64\n",
      " 6   b_w     49080 non-null  int64\n",
      " 7   b_k     49080 non-null  int64\n",
      "dtypes: int64(8)\n",
      "memory usage: 3.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "batting_data = pd.read_csv('2023_complete_batting_data.csv', low_memory=False)\n",
    "\n",
    "# Define input and output columns\n",
    "batting_inputs = [\n",
    "    'left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height',\n",
    "    'attendance', 'temp', 'windspeed', 'season_batting_avg', 'daynight_day', 'daynight_night',\n",
    "    'precip_drizzle', 'precip_none', 'precip_rain', 'precip_snow', 'sky_cloudy', 'sky_dome',\n",
    "    'sky_overcast', 'sky_sunny', 'winddir_fromcf', 'winddir_fromlf', 'winddir_fromrf',\n",
    "    'winddir_ltor', 'winddir_rtol', 'winddir_tocf', 'winddir_tolf', 'winddir_torf', 'winddir_unknown'\n",
    "]\n",
    "batting_outputs = ['b_ab', 'b_h', 'b_d', 'b_t', 'b_hr', 'b_rbi', 'b_w', 'b_k']\n",
    "\n",
    "# Handle missing and categorical data\n",
    "batting_data['precip'] = batting_data['precip'].fillna('none')\n",
    "batting_data = batting_data.dropna()\n",
    "categorical_data = ['daynight', 'precip', 'sky', 'winddir']\n",
    "batting_data = pd.get_dummies(batting_data, columns=categorical_data)\n",
    "\n",
    "# Post-encoding dummy variables\n",
    "b_encoded_variables = [\n",
    "    'daynight_day', 'daynight_night', 'precip_drizzle', 'precip_none', 'precip_rain',\n",
    "    'precip_snow', 'sky_cloudy', 'sky_dome', 'sky_overcast', 'sky_sunny',\n",
    "    'winddir_fromcf', 'winddir_fromlf', 'winddir_fromrf', 'winddir_ltor', 'winddir_rtol',\n",
    "    'winddir_tocf', 'winddir_tolf', 'winddir_torf', 'winddir_unknown'\n",
    "]\n",
    "\n",
    "# Convert non-dummy columns to float\n",
    "non_encoded_columns = [\n",
    "    'left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height',\n",
    "    'attendance', 'temp', 'windspeed', 'season_batting_avg'\n",
    "]\n",
    "batting_data[non_encoded_columns] = batting_data[non_encoded_columns].astype(float)\n",
    "\n",
    "# Convert dummy-encoded columns to float\n",
    "batting_data[b_encoded_variables] = batting_data[b_encoded_variables].astype(int)\n",
    "\n",
    "# Extract input and output data\n",
    "batting_input_data = batting_data[batting_inputs]\n",
    "batting_output_data = batting_data[batting_outputs]\n",
    "\n",
    "# Verify data types\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(batting_input_data.info())\n",
    "print(batting_output_data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_field</th>\n",
       "      <th>center_field</th>\n",
       "      <th>right_field</th>\n",
       "      <th>min_wall_height</th>\n",
       "      <th>max_wall_height</th>\n",
       "      <th>attendance</th>\n",
       "      <th>temp</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>season_era</th>\n",
       "      <th>daynight_day</th>\n",
       "      <th>daynight_night</th>\n",
       "      <th>precip_drizzle</th>\n",
       "      <th>precip_none</th>\n",
       "      <th>precip_rain</th>\n",
       "      <th>precip_snow</th>\n",
       "      <th>sky_cloudy</th>\n",
       "      <th>sky_dome</th>\n",
       "      <th>sky_overcast</th>\n",
       "      <th>sky_sunny</th>\n",
       "      <th>winddir_fromcf</th>\n",
       "      <th>winddir_fromlf</th>\n",
       "      <th>winddir_fromrf</th>\n",
       "      <th>winddir_ltor</th>\n",
       "      <th>winddir_rtol</th>\n",
       "      <th>winddir_tocf</th>\n",
       "      <th>winddir_tolf</th>\n",
       "      <th>winddir_torf</th>\n",
       "      <th>winddir_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>310.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>36049.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.708</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>310.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>36049.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.845</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>310.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>36049.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.436</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>310.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>36049.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.169</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>36049.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   left_field  center_field  right_field  min_wall_height  max_wall_height  \\\n",
       "0       310.0         420.0        302.0              3.0             37.0   \n",
       "1       310.0         420.0        302.0              3.0             37.0   \n",
       "2       310.0         420.0        302.0              3.0             37.0   \n",
       "3       310.0         420.0        302.0              3.0             37.0   \n",
       "4       310.0         420.0        302.0              3.0             37.0   \n",
       "\n",
       "   attendance  temp  windspeed  season_era  daynight_day  daynight_night  \\\n",
       "0     36049.0  38.0       12.0       4.708             1               0   \n",
       "1     36049.0  38.0       12.0       6.845             1               0   \n",
       "2     36049.0  38.0       12.0       3.436             1               0   \n",
       "3     36049.0  38.0       12.0       4.169             1               0   \n",
       "4     36049.0  38.0       12.0       6.000             1               0   \n",
       "\n",
       "   precip_drizzle  precip_none  precip_rain  precip_snow  sky_cloudy  \\\n",
       "0               0            1            0            0           0   \n",
       "1               0            1            0            0           0   \n",
       "2               0            1            0            0           0   \n",
       "3               0            1            0            0           0   \n",
       "4               0            1            0            0           0   \n",
       "\n",
       "   sky_dome  sky_overcast  sky_sunny  winddir_fromcf  winddir_fromlf  \\\n",
       "0         0             0          1               0               0   \n",
       "1         0             0          1               0               0   \n",
       "2         0             0          1               0               0   \n",
       "3         0             0          1               0               0   \n",
       "4         0             0          1               0               0   \n",
       "\n",
       "   winddir_fromrf  winddir_ltor  winddir_rtol  winddir_tocf  winddir_tolf  \\\n",
       "0               0             1             0             0             0   \n",
       "1               0             1             0             0             0   \n",
       "2               0             1             0             0             0   \n",
       "3               0             1             0             0             0   \n",
       "4               0             1             0             0             0   \n",
       "\n",
       "   winddir_torf  winddir_unknown  \n",
       "0             0                0  \n",
       "1             0                0  \n",
       "2             0                0  \n",
       "3             0                0  \n",
       "4             0                0  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitching_input_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For predictions, we are also going to use average valuesand modes. For numerical data we will use the mean. For categorical datas, we will use the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Tune or Neural Network (NN), we are using different numbers. To do so, we will use the gridsearch CV function to process our Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements for Neural Networks\n",
    "import sklearn as sk\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support our multi-output classifier, we needed to create a multioutput evaluation method to score our tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multioutput_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy for multi-output targets.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    accuracies = [\n",
    "        accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "        for i in range(y_true.shape[1])\n",
    "    ]\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "\n",
    "\n",
    "multioutput_scorer = make_scorer(multioutput_accuracy, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitching Model\n",
    "Firstly we will start with our pitching neural network. We start off with scaling our data and reducing our number of dimensions. From there, we will run it through our MLPClassifier Algorithm from Sci-kit learn. We will determine what hyperparameters work best for our neural network by using the GridSearchCV function to get a cross validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline of processes to run through\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "pline = Pipeline([('scaling', sk.preprocessing.StandardScaler()), ('pca', PCA()),\n",
    "                  ('nnet', MultiOutputClassifier(MLPClassifier( early_stopping= True)))])\n",
    "\n",
    "# Defines Parameters to Test\n",
    "param_grid = {\n",
    "    'pca__n_components':[5, 7, 9],\n",
    "    'nnet__estimator__hidden_layer_sizes':[10,20,30],\n",
    "    'nnet__estimator__activation': ['relu'],\n",
    "    'nnet__estimator__alpha':[0.0001,0.01],\n",
    "    'nnet__estimator__max_iter':[500, 1000]\n",
    "}\n",
    "\n",
    "# Subsample the data for grid search (randomly selecting 10,000 samples)\n",
    "gs_pitching_input_data = pitching_input_data.sample(n=10000, random_state=42)\n",
    "gs_pitching_output_data = pitching_output_data.loc[gs_pitching_input_data.index]\n",
    "\n",
    "\n",
    "# Grid Search + Scoring\n",
    "gs = GridSearchCV(pline, param_grid, cv=kf, scoring=multioutput_scorer, n_jobs=-1)\n",
    "\n",
    "# Cross-validate using the subsampled data\n",
    "pitching_nested_score = cross_val_score(gs, gs_pitching_input_data.values, gs_pitching_output_data.values, \n",
    "                                        cv=kf,scoring=multioutput_scorer, n_jobs=-1)\n",
    "\n",
    "print(\"Nested cross-validation scores:\", pitching_nested_score)\n",
    "print(\"Mean Accuracy: \", pitching_nested_score.mean() * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing a grid search, we will now extract the best values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Best Parameters\n",
    "gs.fit(pitching_input_data, pitching_output_data)\n",
    "best_params = gs.best_params_\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "#{'nnet__estimator__activation': 'relu', 'nnet__estimator__alpha': 0.0001, 'nnet__estimator__hidden_layer_sizes': 30, 'pca__n_components': 5} From previous try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Final Algorithm\n",
    "\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(\n",
    "    pitching_input_data.values,  # Ensure NumPy arrays\n",
    "    pitching_output_data.values, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Generate Final Algorithm with the best parameters\n",
    "final_model = Pipeline([\n",
    "    ('scaling', sk.preprocessing.StandardScaler()), \n",
    "    ('pca', PCA(n_components=best_params['pca__n_components'])),\n",
    "    ('nnet', MultiOutputClassifier(MLPClassifier(\n",
    "        activation=best_params['nnet__estimator__activation'],\n",
    "        hidden_layer_sizes=best_params['nnet__estimator__hidden_layer_sizes'],\n",
    "        alpha=best_params['nnet__estimator__alpha'],\n",
    "        max_iter=best_params['nnet__estimator__max_iter'],\n",
    "        early_stopping=True\n",
    "    )))\n",
    "])\n",
    "\n",
    "# Train the final model on the training set\n",
    "final_model.fit(X_train, y_train)\n",
    "print(\"Final model training completed.\")\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "# Compute and print the accuracy\n",
    "test_accuracy = multioutput_accuracy(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate detailed classification reports for each output variable\n",
    "for i, col in enumerate(pitching_output_data.columns):\n",
    "    print(f\"Classification Report for {col}:\")\n",
    "    print(sk.metrics.classification_report(y_test[:, i], y_pred[:, i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pitching_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "print(\"Final model saved as 'pitching_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pitching_input_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the model, we will now evaluate how much of an impact each of the input values has on the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "class MultiOutputPipelineWrapper:\n",
    "    def __init__(self, pipeline, target_index):\n",
    "        self.pipeline = pipeline\n",
    "        self.target_index = target_index\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict only for the specified output\n",
    "        return self.pipeline.predict(X)[:, self.target_index]\n",
    "\n",
    "# Wrap your pipeline for a specific target (e.g., `p_ipouts`)\n",
    "target_index = 0  # Specify the index of the output you want to analyze\n",
    "wrapped_model = MultiOutputPipelineWrapper(final_model, target_index)\n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.Explainer(wrapped_model.predict, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Plot SHAP summary for the analyzed output\n",
    "shap.summary_plot(shap_values, X_test, feature_names=pitching_input_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batting Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline of processes to run through\n",
    "pline2 = Pipeline([('scaling', sk.preprocessing.StandardScaler()), ('pca', PCA()),\n",
    "                   ('nnet', MultiOutputClassifier(MLPClassifier(max_iter=1000, early_stopping=True)))])\n",
    "\n",
    "# Defines Parameters to Test\n",
    "param_grid2 = {\n",
    "    'pca__n_components':[5, 7, 9],\n",
    "    'nnet__estimator__hidden_layer_sizes':[10,20,30],\n",
    "    'nnet__estimator__activation': ['relu'],\n",
    "    'nnet__estimator__alpha':[0.0001,0.01],\n",
    "    'nnet__estimator__max_iter':[500, 1000]\n",
    "}\n",
    "\n",
    "# Subsample the data for grid search\n",
    "gs_batting_input_data = batting_input_data.sample(10000, random_state=42)\n",
    "gs_batting_output_data = batting_output_data.sample(10000, random_state=42)\n",
    "\n",
    "# Grid Search + Scoring\n",
    "gs2 = GridSearchCV(pline2, param_grid2, cv=5, scoring=multioutput_scorer, n_jobs=-1)\n",
    "\n",
    "# Cross-validate using the subsampled data\n",
    "batting_nested_score = cross_val_score(gs2, \n",
    "                                       gs_batting_input_data.values, \n",
    "                                       gs_batting_output_data.values, \n",
    "                                       cv=3, \n",
    "                                       scoring=multioutput_scorer, \n",
    "                                       n_jobs=-1)\n",
    "\n",
    "print(\"Nested cross-validation scores:\", batting_nested_score)\n",
    "print(\"Mean Accuracy: \", batting_nested_score.mean() * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Best Parameters\n",
    "gs2.fit(batting_input_data, batting_output_data)\n",
    "best_params2 = gs2.best_params_\n",
    "print(best_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train2, X_test2, y_train2, y_test2 = sk.model_selection.train_test_split(\n",
    "    batting_input_data.values,  # Ensure NumPy arrays\n",
    "    batting_output_data.values, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Generate Final Algorithm with the best parameters\n",
    "final_model2 = Pipeline([\n",
    "    ('scaling', sk.preprocessing.StandardScaler()), \n",
    "    ('pca', PCA(n_components=best_params2['pca__n_components'])),\n",
    "    ('nnet', MultiOutputClassifier(MLPClassifier(\n",
    "        activation=best_params2['nnet__estimator__activation'],\n",
    "        hidden_layer_sizes=best_params2['nnet__estimator__hidden_layer_sizes'],\n",
    "        alpha=best_params2['nnet__estimator__alpha'],\n",
    "        max_iter=1000,\n",
    "        early_stopping=True\n",
    "    )))\n",
    "])\n",
    "\n",
    "# Train the final model on the training set\n",
    "final_model2.fit(X_train2, y_train2)\n",
    "print(\"Final model training completed.\")\n",
    "\n",
    "# Test the model and compute predictions\n",
    "y_pred2 = final_model2.predict(X_test2)\n",
    "\n",
    "# Compute and print the accuracy\n",
    "test_accuracy2 = multioutput_accuracy(y_test2, y_pred2)\n",
    "print(f\"Test Accuracy: {test_accuracy2 * 100:.2f}%\")\n",
    "\n",
    "# Generate detailed classification reports for each output variable\n",
    "for i, col in enumerate(batting_output_data.columns):\n",
    "    print(f\"Classification Report for {col}:\")\n",
    "    print(sk.metrics.classification_report(y_test2[:, i], y_pred2[:, i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batting_model = pickle.load(open('batting_model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "class MultiOutputPipelineWrapper:\n",
    "    def __init__(self, pipeline, target_index):\n",
    "        self.pipeline = pipeline\n",
    "        self.target_index = target_index\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict only for the specified output\n",
    "        return self.pipeline.predict(X)[:, self.target_index]\n",
    "\n",
    "# Wrap your pipeline for a specific target (e.g., `p_ipouts`)\n",
    "target_index = 0  # Specify the index of the output you want to analyze\n",
    "wrapped_model = MultiOutputPipelineWrapper(batting_model, target_index)\n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.Explainer(wrapped_model.predict, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Plot SHAP summary for the analyzed output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, feature_names=pitching_input_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"batting_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_model2, f)\n",
    "print(\"Final model saved as 'batting_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Model\n",
    "\n",
    "After tuning and training the model, we will now use the model. You can load in the model by using the Pickle load method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitching_model = pickle.load(open('pitching_model.pkl', 'rb'))\n",
    "print(\"Pitching model imported from 'pitching_model.pkl' as pitching_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the Player Class\n",
    "\n",
    "We created the base player class to have the player object contain all the non-player based classification data and then have it separate with subclasses as pitchers and batters. We included default values for environmental and player stats.\n",
    "\n",
    "For batters, we have an average batting average of 0.226\n",
    "\n",
    "For pitchers, we have an average season ERA of 5.870"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For environmental variables, we used either the mean of numerical values or used the mode of categorical variables. \n",
    "This provided us with stadium values of the following:\n",
    "\n",
    "Average Column Values:  \n",
    "left_field         331.833333  \n",
    "center_field       404.166667  \n",
    "right_field        328.333333  \n",
    "min_wall_height      7.553333  \n",
    "max_wall_height     14.266667  \n",
    "  \n",
    "We also got game variables of the following:  \n",
    "\n",
    "Average Values for Numeric Columns:  \n",
    "attendance    29356.347087  \n",
    "temp             72.413835  \n",
    "windspeed         6.466828  \n",
    "  \n",
    "Most Frequent Values for Categorical Columns:  \n",
    "daynight      night  \n",
    "precip         none  \n",
    "sky          cloudy  \n",
    "winddir     unknown  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    left_field, center_field, right_field, min_wall_height, max_wall_height, attendance, temp, windspeed = (-1,) * 8\n",
    "    daynight, precip, winddir = ('',)*3\n",
    "    def __init__(self, name = 'unknown', lf = 331.833, cf = 404.167, rf = 328.333, min_wh = 7.553, max_wh = 14.267, att = 29356,\n",
    "                 t = 74.41, ws = 6.466, dn = 'night', pp = 'none', s = 'cloudy', wd = 'unknown'):\n",
    "        self.name = name\n",
    "        self.left_field = lf\n",
    "        self.center_field = cf\n",
    "        self.right_field = rf\n",
    "        self.min_wall_height= min_wh\n",
    "        self.max_wall_height = max_wh \n",
    "        self.attendance = att\n",
    "        self.temp = t \n",
    "        self.windspeed = ws\n",
    "        self.daynight = dn \n",
    "        self.precip = pp\n",
    "        self.winddir = wd\n",
    "        self.stat = -1\n",
    "        self.sky = s\n",
    "    \n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "\n",
    "\n",
    "    def set_environment_data(self, ballpark_name):\n",
    "        ballparks = pd.read_csv('ballparks.csv')\n",
    "        name_column = 'ballpark'\n",
    "        environment_columns = ['left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height']\n",
    "\n",
    "        # Filter the DataFrame for the specified ballpark\n",
    "        filtered_row = ballparks[ballparks[name_column] == ballpark_name]\n",
    "\n",
    "        if filtered_row.empty:\n",
    "            print( f\"Ballpark '{ballpark_name}' not found.\")\n",
    "            pass\n",
    "\n",
    "        # Extract the relevant environment columns and set them as instance attributes\n",
    "        environment_data = filtered_row.iloc[0]\n",
    "        self.left_field = environment_data['left_field']\n",
    "        self.center_field = environment_data['center_field']\n",
    "        self.right_field = environment_data['right_field']\n",
    "        self.min_wall_height = environment_data['min_wall_height']\n",
    "        self.max_wall_height = environment_data['max_wall_height']\n",
    "\n",
    "    #one-hot-encode helper method\n",
    "    def one_hot_encode(self, value, categories):\n",
    "        return [1 if value == category else 0 for category in categories]\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Base numeric features\n",
    "        features = [\n",
    "            self.left_field, \n",
    "            self.center_field, \n",
    "            self.right_field, \n",
    "            self.min_wall_height, \n",
    "            self.max_wall_height, \n",
    "            self.attendance, \n",
    "            self.temp, \n",
    "            self.windspeed,\n",
    "            self.stat #for batter or pitcher\n",
    "            \n",
    "        ]\n",
    "\n",
    "        # One-hot encode categorical variables\n",
    "        features += self.one_hot_encode(self.daynight, ['day', 'night'])\n",
    "        features += self.one_hot_encode(self.precip, ['drizzle', 'none', 'rain', 'snow'])\n",
    "        features += self.one_hot_encode(self.sky, [\n",
    "            'sky_cloudy', 'sky_dome', 'sky_overcast', 'sky_sunny'\n",
    "        ])\n",
    "        features += self.one_hot_encode(self.winddir, [\n",
    "            'fromcf', 'fromlf', 'fromrf', 'ltor', 'rtol', 'tocf', 'tolf', 'torf', 'unknown'\n",
    "        ])\n",
    "\n",
    "        return iter(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pitcher(Player):\n",
    "    def __init__(self, era=5.870, **kwargs):\n",
    "        super().__init__(**kwargs)  # Pass all parent arguments to Player\n",
    "        self.stat = era  # Set ERA as the stat\n",
    "\n",
    "\n",
    "class Batter(Player):\n",
    "    def __init__(self, batting_avg=0.226, **kwargs):\n",
    "        super().__init__(**kwargs)  # Pass all parent arguments to Player\n",
    "        self.stat = batting_avg  # Set batting average as the stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = Pitcher()\n",
    "pred =pitching_model.predict(np.array(list(bob)).reshape(1,-1))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get players season average (era if pitcher, batting avg if batter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = input(\"Enter a player's name: \").lower()\n",
    "\n",
    "def get_season_avg(name):\n",
    "\n",
    "    pitching_file = '2023_full_pitching_stats_cleaned.csv'\n",
    "    batting_file = '2023_full_batting_stats_cleaned.csv'\n",
    "\n",
    "    pitching_data = pd.read_csv(pitching_file)\n",
    "    batting_data = pd.read_csv(batting_file)\n",
    "\n",
    "    name_column = 'id'\n",
    "    batting_avg_column = 'season_batting_avg'\n",
    "    era_avg_column = 'season_era'\n",
    "\n",
    "    batting_data[name_column] = batting_data[name_column].str.lower()\n",
    "    pitching_data[name_column] = pitching_data[name_column].str.lower()\n",
    "    \n",
    "    if name in batting_data[name_column].values and name in pitching_data[name_column].values:\n",
    "        batter_avg = batting_data[batting_data[name_column] == name][batting_avg_column].values[0]\n",
    "        era_avg = pitching_data[pitching_data[name_column] == name][era_avg_column].values[0]\n",
    "        spec = input(\"Pitcher or Batter: (p or b)\").lower()\n",
    "        if spec == 'p':\n",
    "            return Pitcher(era = float(era_avg))\n",
    "        else:\n",
    "            return Batter(batting_avg = float(batter_avg))\n",
    "\n",
    "    elif name in batting_data[name_column].values:\n",
    "        batter_avg = batting_data[batting_data[name_column] == name][batting_avg_column].values[0]\n",
    "        return Batter(batting_avg = float(batter_avg))\n",
    "\n",
    "    elif name in pitching_data[name_column].values:\n",
    "        era_avg = pitching_data[pitching_data[name_column] == name][era_avg_column].values[0]\n",
    "        return Pitcher(era = float(era_avg))\n",
    "    \n",
    "    else:\n",
    "        spec = input(\"Pitcher or Batter: (p or b)\").lower()\n",
    "        if spec == 'p':\n",
    "            return Pitcher()\n",
    "        else:\n",
    "            return Batter()\n",
    "        \n",
    "\n",
    "jose = get_season_avg(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pitcher(Player):\n",
    "    def __init__(self, era=3.5, **kwargs):\n",
    "        super().__init__(**kwargs)  # Pass all parent arguments to Player\n",
    "        self.stat = era  # Set ERA as the stat\n",
    "\n",
    "    def readout(self):\n",
    "        \n",
    "        p1 = pitching_model.predict(np.array(list(self)).reshape(1,-1)).tolist()\n",
    "        p1 = sum(p1,[])\n",
    "\n",
    "        p1pitchedouts= p1[0]\n",
    "        p1hitsallowed = p1[1]\n",
    "        p1runsgivneup = p1[2]\n",
    "        p1earnedrunsagaisnt = p1[3]\n",
    "        p1causedwalks = p1[4]\n",
    "        p1thrownstrikes = p1[5]\n",
    "        p1hitbypitch = p1[6]\n",
    "        p1wildpitch = p1[7]\n",
    "        \n",
    "        #average time in game for first player\n",
    "        \n",
    "        p1inningstopitch = p1pitchedouts/3\n",
    "        \n",
    "        # strikeouts per walk\n",
    "        p1soutper9 = p1[5]/p1[4]\n",
    "        \n",
    "        # WHIP (walks+hit  / innigs pithced)\n",
    "        p1walktostrike = (p1[4] + p1[1]) / p1inningstopitch\n",
    "        \n",
    "        sigstring = 'nice'\n",
    "        \n",
    "        wind = self.windspd\n",
    "        if wind > 15:\n",
    "            windstring = 'windy'           \n",
    "        \n",
    "            \n",
    "        elif windstring == 'windy':\n",
    "              sigstring = windstring\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        img = mpimg.imread('pitcherimage.png')\n",
    "        \n",
    "        pc = f'IP: {p1inningstopitch}'\n",
    "        sw = f'KK/BB: {p1soutper9}'\n",
    "        wh = f'WHIP: {p1walktostrike}'\n",
    "        \n",
    "        ax.imshow(img, aspect='auto', extent=[-1, 1, -1, 1])\n",
    "        \n",
    "        annotations = [pc, sw, wh]\n",
    "        \n",
    "        # Calculate vertical spacing: assuming we want equal spacing, we divide the area from bottom to top\n",
    "        spacing = 0.1  # Adjust this for how much space you want between the lines\n",
    "        base_y = -.356  # Starting point for the first annotation from the bottom\n",
    "        # Number of annotations to be placed\n",
    "        num_annotations = len(annotations)\n",
    "        \n",
    "        # Loop over annotations to place them\n",
    "        for i, annotation in enumerate(annotations):\n",
    "            # Adjust the y position for each annotation (spacing vertically)\n",
    "            y_position = base_y + i * spacing\n",
    "            ax.text(.456, y_position, annotation, ha='right', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Batter(Player):\n",
    "    def __init__(self, batting_avg=0.275, **kwargs):\n",
    "        super().__init__(**kwargs)  # Pass all parent arguments to Player\n",
    "        self.stat = batting_avg  # Set batting average as the stat\n",
    "\n",
    "    def readout(self):\n",
    "        p1 = batting_model.predict(np.array(list(self)).reshape(1,-1)).tolist()\n",
    "        p1 = sum(p1,[])\n",
    "        \n",
    "        \n",
    "\n",
    "        p1onebaseA = p1[1] - p1[2] - p1[3]\n",
    "        p1twobaseA = p1[2]\n",
    "        p1threebaseA = p1[3]\n",
    "        p1fourbaseA = p1[4]\n",
    "        \n",
    "        #batting average for first player\n",
    "        p1battavg = p1[1] / p1[0]\n",
    "        \n",
    "        # strikeout percentage for first player\n",
    "        p1soutper = p1[7]/p1[0]\n",
    "        \n",
    "        # walk to strikeout ratio for first player\n",
    "        p1walktostrike = p1[6] /p1[7]\n",
    "           \n",
    "        sigstring = 'nice' \n",
    "        windstring = 'none'\n",
    "        if self.windspeed > 8:\n",
    "            windstring = 'windy'\n",
    "        \n",
    "        if windstring == 'windy':\n",
    "            sigstring = windstring\n",
    "    \n",
    "               \n",
    "\n",
    "        flattitle = f\"Expected perfomance of p1.name at STADIUM on a {sigstring} {self.daynight}\"\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "        #Display the baseball diamond image\n",
    "        boardcoord = (.35,-.7)\n",
    "        boxcoord = (-.845,.70)\n",
    "        namecoord = (.35,.65)\n",
    "        img = mpimg.imread('batterpov.png')\n",
    "          \n",
    "        ax.imshow(img, aspect='auto', extent=[-1, 1, -1, 1])\n",
    "\n",
    "        ax.set_axis_off()\n",
    "        quickread = f\"AVG: {p1battavg} BB/K: {p1walktostrike} RBI: {p1[5]}\"\n",
    "        windread = f'{self.windspeed} MPH'\n",
    "        nameread = f'{self.name}'\n",
    "\n",
    "        ax.annotate(quickread,xy = boardcoord, xytext = boardcoord, color ='white')\n",
    "        ax.annotate(windread,xy = boxcoord, xytext = boxcoord, color ='white',fontsize = 8)\n",
    "        ax.annotate(nameread,xy = namecoord, xytext = namecoord, color = 'white',fontsize = 8)\n",
    "\n",
    "    \n",
    "    def compare(self,player2):\n",
    "        p1 = batting_model.predict(np.array(list(self)).reshape(1,-1)).tolist()\n",
    "        p1 = sum(p1,[])\n",
    "        \n",
    "        \n",
    "\n",
    "        p1onebaseA = p1[1] - p1[2] - p1[3]\n",
    "        p1twobaseA = p1[2]\n",
    "        p1threebaseA = p1[3]\n",
    "        p1fourbaseA = p1[4]\n",
    "        \n",
    "        #batting average for first player\n",
    "        p1battavg = p1[1] / p1[0]\n",
    "        \n",
    "        # strikeout percentage for first player\n",
    "        p1soutper = p1[7]/p1[0]\n",
    "        \n",
    "        # walk to strikeout ratio for first player\n",
    "        p1walktostrike = p1[6] /p1[7]\n",
    "           \n",
    "        sigstring = 'nice' \n",
    "        windstring = 'none'\n",
    "        if self.windspeed > 8:\n",
    "            windstring = 'windy'\n",
    "        \n",
    "        if windstring == 'windy':\n",
    "            sigstring = windstring\n",
    "    \n",
    "        \n",
    "        \n",
    "        #for comparsion to follow\n",
    "        pathannotA = (p1onebaseA, p1twobaseA,p1threebaseA,p1fourbaseA)\n",
    "\n",
    "        actualcompa = [p1battavg,p1walktostrike,p1[5]]\n",
    "        \n",
    "        # player2.battingoutput() = some list with the same indexes as above\n",
    "        \n",
    "        p2 = batting_model.predict(np.array(list(player2)).reshape(1,-1)).tolist()\n",
    "        p2 = sum(p2,[])\n",
    "        \n",
    "        \n",
    "\n",
    "        p2onebaseA = p2[1] - p2[2] - p2[3]\n",
    "        p2twobaseA = p2[2]\n",
    "        p2threebaseA = p2[3]\n",
    "        p2fourbaseA = p2[4]\n",
    "        \n",
    "        #batting average for first player\n",
    "        p2battavg = p2[1] / p2[0]\n",
    "        \n",
    "        # strikeout percentage for first player\n",
    "        p2soutper = p2[7]/p2[0]\n",
    "        \n",
    "        # walk to strikeout ratio for first player\n",
    "        p2walktostrike = p2[6] /p2[7]\n",
    "           \n",
    "        sigstring = 'nice' \n",
    "        windstring = 'none'\n",
    "        if self.windspeed > 8:\n",
    "            windstring = 'windy'\n",
    "        \n",
    "        if windstring == 'windy':\n",
    "            sigstring = windstring\n",
    "        \n",
    "        actualcompb = [p2battavg,p2walktostrike,p2[5]]\n",
    "        \n",
    "        # comparing to color annotations\n",
    "\n",
    "        \n",
    "        \n",
    "        # Function to compare values from both lists and assign colors\n",
    "        def compare_and_assign_colors(actualcompa, actualcompb):\n",
    "            colors = []  # List to store colors based on the comparison\n",
    "            for val1, val2 in zip(actualcompa, actualcompb):\n",
    "                if val1 == val2:\n",
    "                    colors.append('white')  # Equal values -> green\n",
    "                elif val1 > val2:\n",
    "                    colors.append('green')    # list1 value is greater -> red\n",
    "                else:\n",
    "                    colors.append('red')   # list2 value is greater -> blue\n",
    "            return colors\n",
    "        \n",
    "        # Assign colors based on the comparison\n",
    "        colorsa = compare_and_assign_colors(actualcompa, actualcompb)\n",
    "        colorsb = compare_and_assign_colors(actualcompb, actualcompa)  # Reverse comparison for list2 if needed\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        #comptitle = f\"Expected performance of p1.name and player2.name at {location} on a {sigstring} {p1.daynight}\"\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        boardcoord = (.35,-.7)\n",
    "        boxcoord = (-.845,.70)\n",
    "        namecoord = (.35,.65)\n",
    "        img = mpimg.imread('batterpov.png')\n",
    "          \n",
    "        ax.imshow(img, aspect='auto', extent=[-1, 1, -1, 1])\n",
    "\n",
    "        ax.set_axis_off()\n",
    "        windread = f'{self.windspeed} MPH'\n",
    "        nameread = f'{self.name}'\n",
    "\n",
    "        # Annotations with different colors for each part of the string\n",
    "        ax.text(.35, -.7, f\"AVG: {p1battavg}\", color=colorsa[0], fontsize=10)  # Red for AVG\n",
    "        ax.text(.6, -.7, f\"BB/K: {p1walktostrike}\", color=colorsa[1], fontsize=10)  # Blue for BB/K\n",
    "        ax.text(.85, -.7, f\"RBI: {p1[5]}\", color=colorsa[2], fontsize=10)  # Green for RBI\n",
    "        \n",
    "        ax.annotate(windread,xy = boxcoord, xytext = boxcoord, color ='white',fontsize = 8)\n",
    "        ax.annotate(nameread,xy = namecoord, xytext = namecoord, color = 'white',fontsize = 8)\n",
    "        #REPEAT FOR PLAYER 2\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        boardcoord = (.35,-.7)\n",
    "        boxcoord = (-.845,.70)\n",
    "        namecoord = (.35,.65)\n",
    "        img = mpimg.imread('batterpov.png')\n",
    "          \n",
    "        ax.imshow(img, aspect='auto', extent=[-1, 1, -1, 1])\n",
    "\n",
    "        ax.set_axis_off()\n",
    "        windread = f'{player2.windspeed} MPH'\n",
    "        nameread = f'{player2.name}'\n",
    "\n",
    "        # Annotations with different colors for each part of the string\n",
    "        ax.text(.35, -.7, f\"AVG: {p2battavg}\", color=colorsb[0], fontsize=10)  # Red for AVG\n",
    "        ax.text(.6, -.7, f\"BB/K: {p2walktostrike}\", color=colorsb[1], fontsize=10)  # Blue for BB/K\n",
    "        ax.text(.85, -.7, f\"RBI: {p2[5]}\", color=colorsa[2], fontsize=10)  # Green for RBI\n",
    "        \n",
    "        ax.annotate(windread,xy = boxcoord, xytext = boxcoord, color ='white',fontsize = 8)\n",
    "        ax.annotate(nameread,xy = namecoord, xytext = namecoord, color = 'white',fontsize = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Get players season average (era if pitcher, batting avg if batter)\n",
    "name = input(\"Enter a player's name: \").lower()\n",
    "\n",
    "def get_season_avg(name):\n",
    "\n",
    "    pitching_file = '2023_full_pitching_stats_cleaned.csv'\n",
    "    batting_file = '2023_full_batting_stats_cleaned.csv'\n",
    "\n",
    "    pitching_data = pd.read_csv(pitching_file)\n",
    "    batting_data = pd.read_csv(batting_file)\n",
    "\n",
    "    name_column = 'id'\n",
    "    batting_avg_column = 'season_batting_avg'\n",
    "    era_avg_column = 'season_era'\n",
    "\n",
    "    batting_data[name_column] = batting_data[name_column].str.lower()\n",
    "    pitching_data[name_column] = pitching_data[name_column].str.lower()\n",
    "    \n",
    "    if name in batting_data[name_column].values and name in pitching_data[name_column].values:\n",
    "        batter_avg = batting_data[batting_data[name_column] == name][batting_avg_column].values[0]\n",
    "        era_avg = pitching_data[pitching_data[name_column] == name][era_avg_column].values[0]\n",
    "        spec = input(\"Pitcher or Batter: (p or b)\").lower()\n",
    "        if spec == 'p':\n",
    "            return Pitcher(era = float(era_avg))\n",
    "        else:\n",
    "            return Batter(batting_avg = float(batter_avg))\n",
    "\n",
    "    elif name in batting_data[name_column].values:\n",
    "        batter_avg = batting_data[batting_data[name_column] == name][batting_avg_column].values[0]\n",
    "        return Batter(batting_avg = float(batter_avg))\n",
    "\n",
    "    elif name in pitching_data[name_column].values:\n",
    "        era_avg = pitching_data[pitching_data[name_column] == name][era_avg_column].values[0]\n",
    "        return Pitcher(era = float(era_avg))\n",
    "    \n",
    "    else:\n",
    "        spec = input(\"Pitcher or Batter: (p or b)\").lower()\n",
    "        if spec == 'p':\n",
    "            return Pitcher()\n",
    "        else:\n",
    "            return Batter()\n",
    "        \n",
    "\n",
    "jose = get_season_avg(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = Batter()\n",
    "print(list(jose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob_data = np.array(list(bob)).reshape(1,-1)\n",
    "pred_class = pitching_model.predict(bob_data)\n",
    "pred_prob = pitching_model.predict_proba(bob_data)\n",
    "# ['left_field', 'center_field', 'right_field','min_wall_height','max_wall_height',\n",
    "                                    # 'attendance','temp','windspeed','season_era', 'daynight_day', 'daynight_night', 'precip_drizzle', 'precip_none', 'precip_rain', \n",
    "                                    # 'precip_snow', 'sky_cloudy', 'sky_dome', 'sky_overcast', 'sky_sunny', 'winddir_fromcf', 'winddir_fromlf', 'winddir_fromrf', 'winddir_ltor', \n",
    "                                    # 'winddir_rtol', 'winddir_tocf', 'winddir_tolf', 'winddir_torf', 'winddir_unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_class)\n",
    "print(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = Batter(name=\"Christian Encarnacion-Strand\")\n",
    "jack = Batter(name='jack')\n",
    "list(bob)\n",
    "\n",
    "k = batting_model.predict(np.array(list(bob)).reshape(1,-1)).tolist()\n",
    "\n",
    "print(k)\n",
    "\n",
    "k = sum(k,[])\n",
    "\n",
    "print(k)\n",
    "\n",
    "print(bob.windspeed)\n",
    "#bob.readout()\n",
    "bob.compare(jack)\n",
    "jill = Pitcher('jill')\n",
    "jill.readout()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
