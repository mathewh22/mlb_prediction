{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLB Predictor Project\n",
    "\n",
    "Group 21, Plotters for Success\n",
    "\n",
    "Gerardo Skrut, Victor Gikunda, Mathew Huang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to importing the data, we cleaned and explored the existing data.\n",
    "\n",
    "Finally, After we consolidated two datasets with pitching and batting, we are going to separate each portion to inputs and outputs respectively. \n",
    "\n",
    "Our inputs overall would include Left Field, Right Field, and Centerfield Distance, Maximum and minimum wall height, Day/night, Attendance, Precipitation, Sky Condition, Temperature, Wind Direction, and Wind Speed. \n",
    "\n",
    "For Pitching specifically, we will be using the pitcher's **Season ERA** from the 2023 Season. \n",
    "\n",
    "For Batting Specifically, we will be using the batter's **Season Batting Average** from the 2023 Season.\n",
    "\n",
    "Our outputs would be game specific statistics. \n",
    "\n",
    "For Pitching, we would have the number of Hits Allowed, Runs Allowed, Earned Runs, Walks Given, Hit by Pitches, and Wild Pitches.\n",
    "\n",
    "For Batting, we would have the number of Hits, Doubles, Triples, Home Runs, RBIs, Walks, and Strikeouts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ballpark Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Column Values:\n",
      "left_field         331.833333\n",
      "center_field       404.166667\n",
      "right_field        328.333333\n",
      "min_wall_height      7.553333\n",
      "max_wall_height     14.266667\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load ballparks data\n",
    "data = pd.read_csv('ballparks.csv')\n",
    "\n",
    "# Filter for relevant columns\n",
    "columns_to_keep = ['team_name', 'ballpark', 'left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height']\n",
    "data_filtered = data[columns_to_keep]\n",
    "\n",
    "# Calculate average for numeric columns\n",
    "average_values = data_filtered[['left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height']].mean()\n",
    "\n",
    "# Print the average values\n",
    "print(\"Average Column Values:\")\n",
    "print(average_values)\n",
    "\n",
    "# Save the filtered data\n",
    "data_filtered.to_csv('2023_filtered_ballpark_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Information Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Values for Numeric Columns:\n",
      "attendance    29356.347087\n",
      "temp             72.413835\n",
      "windspeed         6.466828\n",
      "dtype: float64\n",
      "\n",
      "Most Frequent Values for Categorical Columns:\n",
      "daynight      night\n",
      "precip         none\n",
      "sky          cloudy\n",
      "winddir     unknown\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load game info data\n",
    "data = pd.read_csv('2023gameinfo.csv')\n",
    "\n",
    "# Filter for relevant columns\n",
    "columns_to_keep = ['gid', 'daynight', 'attendance', 'precip', 'sky', 'temp', 'winddir', 'windspeed']\n",
    "data_filtered = data[columns_to_keep]\n",
    "\n",
    "# Calculate average for numeric columns\n",
    "numeric_columns = ['attendance', 'temp', 'windspeed']\n",
    "average_values = data_filtered[numeric_columns].mean()\n",
    "\n",
    "# Find the most frequent value for categorical columns\n",
    "categorical_columns = ['daynight', 'precip', 'sky', 'winddir']\n",
    "most_frequent_values = data_filtered[categorical_columns].mode().iloc[0]\n",
    "\n",
    "# Print results\n",
    "print(\"Average Values for Numeric Columns:\")\n",
    "print(average_values)\n",
    "print(\"\\nMost Frequent Values for Categorical Columns:\")\n",
    "print(most_frequent_values)\n",
    "\n",
    "# Save filtered data\n",
    "data_filtered.to_csv('2023_filtered_gameinfo_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batting Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''filters for relevant batting player data'''\n",
    "# load csv file\n",
    "data = pd.read_csv('2023batting.csv')\n",
    "\n",
    "# filters for columns with relevant data\n",
    "columns_to_keep = ['gid', 'id', 'team', 'b_ab', 'b_h', 'b_d', 'b_t', 'b_hr', 'b_rbi', 'b_w', 'b_k', 'date', 'wl']  \n",
    "\n",
    "# creates a new data frame\n",
    "data_filtered = data[columns_to_keep]\n",
    "\n",
    "# save new csv file\n",
    "data_filtered.to_csv('2023_filtered_batting_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dataset includes batting data for players who do not bat, this filters out those players who had zero at bats (i.e. pitchers)'''\n",
    "\n",
    "updated_rows = []\n",
    "with open('2023_filtered_batting_data.csv', 'r') as data_file:  \n",
    "    data_reader = csv.reader(data_file)\n",
    "    header = next(data_reader)  \n",
    "    updated_rows.append(header)\n",
    "\n",
    "    # reads each row\n",
    "    for row in data_reader:\n",
    "        # checks if players number of plate appearances is zero\n",
    "        if int(row[3]) == 0:\n",
    "            # skips if plate appearance is equal to zero  \n",
    "            continue  \n",
    "\n",
    "        # appends data if not    \n",
    "        updated_rows.append(row)\n",
    "\n",
    "# creates a new csv file with updated data\n",
    "with open('2023_batting_data_cleaned.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.writer(updated_file)\n",
    "    writer.writerows(updated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Merges relevant data into a one csv file'''\n",
    "\n",
    "# loads ball park data into a dictionary\n",
    "stadium_data = {}\n",
    "with open('2023_filtered_ballpark_data.csv', 'r') as stadium_file:  \n",
    "    stadium_reader = csv.DictReader(stadium_file)\n",
    "    for row in stadium_reader:\n",
    "        # uses the team name as the key for each entry\n",
    "        team_name = row['team_name']  \n",
    "        # stores the data for the corresponding team\n",
    "        stadium_data[team_name] = row  \n",
    "\n",
    "# loads game info data into a dictionary       \n",
    "gameinfo_data = {}\n",
    "with open('2023_filtered_gameinfo_data.csv', 'r') as gameinfo_file:  \n",
    "    gameinfo_reader = csv.DictReader(gameinfo_file)\n",
    "    for row in gameinfo_reader:\n",
    "        # uses the game id as the key for each entry\n",
    "        gid = row['gid']  \n",
    "        # stores the data for the corresponding game id \n",
    "        gameinfo_data[gid] = row  \n",
    "        \n",
    "\n",
    "# read batting data, merge with ball park data, store updated rows\n",
    "updated_rows = []\n",
    "with open('2023_batting_data_cleaned.csv', 'r') as game_log_file: \n",
    "    batting_log = csv.reader(game_log_file)\n",
    "    # captures the header row\n",
    "    header = next(batting_log)\n",
    "\n",
    "    # extracts column names of the ball park data\n",
    "    stadium_columns = list(stadium_data.values())[0].keys() \n",
    "    # extracts column names of the game info data, excluding 'gid' since it already exist in the dataset \n",
    "    gameinfo_columns = list(gameinfo_data.values())[0].keys()\n",
    "    new_gameinfo_columns = []\n",
    "    \n",
    "    for col in gameinfo_columns:\n",
    "        if col != 'gid':\n",
    "            new_gameinfo_columns.append(col)\n",
    "\n",
    "    # appens the original header with additional stadium and game info columns           \n",
    "    updated_rows.append(header + list(stadium_columns) + new_gameinfo_columns)          \n",
    "\n",
    "    # iterates through each row of the batting log data      \n",
    "    for row in batting_log:\n",
    "        \n",
    "        # extracts game id\n",
    "        gid = row[0] \n",
    "        # extracts first three letters of the game id, which is the team id\n",
    "        team_id = gid[:3]\n",
    "\n",
    "        # checks if the team id exists in the stadium data        \n",
    "        if team_id in stadium_data:\n",
    "            # retieves ball park data for that team and appends it to the row\n",
    "            # this can be done because the first three characters of the game id represent the team id of the home team \n",
    "            stadium_info = stadium_data[team_id]\n",
    "            row.extend(stadium_info.values())\n",
    "        \n",
    "        # checks if game id is in the game info data\n",
    "        if gid in gameinfo_data:\n",
    "            \n",
    "            # retrieves game info data to corresponding game id\n",
    "            gameinfo = gameinfo_data[gid]\n",
    "            updated_gameinfo = []\n",
    "            # removes the duplicate game id column and appends the other values\n",
    "            for key, value in gameinfo.items():\n",
    "                if key != 'gid':\n",
    "                    updated_gameinfo.append(value)\n",
    "            \n",
    "            # adds game info data to current row\n",
    "            row.extend(updated_gameinfo)\n",
    "\n",
    "        # adds new data to row   \n",
    "        updated_rows.append(row)\n",
    "\n",
    "\n",
    "# new csv file output with updated data\n",
    "with open('2023_merged_batting_data.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.writer(updated_file)\n",
    "    writer.writerows(updated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Batting Average: 0.226\n"
     ]
    }
   ],
   "source": [
    "'''adds a row with calculated season average for each player since the data set only includes specific game log data'''\n",
    " \n",
    "# Dictionary to store season totals\n",
    "season_avg_data = {}\n",
    "\n",
    "# Process input file to calculate season totals for each player\n",
    "with open('2023_merged_batting_data.csv', 'r') as stat_file:\n",
    "    stat_reader = csv.DictReader(stat_file)\n",
    "    header = stat_reader.fieldnames + ['season_batting_avg']\n",
    "\n",
    "    for row in stat_reader:\n",
    "        name = row['id']\n",
    "        bats = int(row['b_ab'])\n",
    "        hits = int(row['b_h'])\n",
    "\n",
    "        # Update season totals for the player\n",
    "        if name in season_avg_data:\n",
    "            season_avg_data[name]['total_at_bats'] += bats\n",
    "            season_avg_data[name]['total_hits'] += hits\n",
    "        else:\n",
    "            season_avg_data[name] = {'total_at_bats': bats, 'total_hits': hits}\n",
    "\n",
    "# Prepare updated rows with calculated season batting averages\n",
    "updated_rows = []\n",
    "with open('2023_merged_batting_data.csv', 'r') as stat_file:\n",
    "    stat_reader = csv.DictReader(stat_file)\n",
    "    for row in stat_reader:\n",
    "        name = row['id']\n",
    "        total_bats = season_avg_data[name]['total_at_bats']\n",
    "        total_hits = season_avg_data[name]['total_hits']\n",
    "\n",
    "        # Calculate player's season batting average\n",
    "        batting_average = total_hits / total_bats\n",
    "        row['season_batting_avg'] = f\"{batting_average:.3f}\"\n",
    "\n",
    "        updated_rows.append(row)\n",
    "\n",
    "# Write updated data to a new CSV file\n",
    "with open('2023_complete_batting_data.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.DictWriter(updated_file, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(updated_rows)\n",
    "\n",
    "# Calculate overall average batting average (NEW ADDITION)\n",
    "batting_averages = [\n",
    "    stats['total_hits'] / stats['total_at_bats']\n",
    "    for stats in season_avg_data.values()\n",
    "    if stats['total_at_bats'] > 0  # Avoid division by zero\n",
    "]\n",
    "\n",
    "average_batting_avg = sum(batting_averages) / len(batting_averages) if batting_averages else 0\n",
    "\n",
    "# Print the overall average batting average (does not affect functionality)\n",
    "print(f\"Average Batting Average: {average_batting_avg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''formats finalzied dataset properly'''\n",
    "\n",
    "# creates a mapping of player IDs to their full names\n",
    "name_id_map = {}\n",
    "with open('2023allplayers.csv', 'r') as name_file:  \n",
    "    name_reader = csv.DictReader(name_file)\n",
    "    for row in name_reader:\n",
    "        # combines first and last name columns to form the full name and maps it respectively \n",
    "        name_id_map[row['id']] = f\"{row['first']} {row['last']}\"\n",
    "\n",
    "# reads the baseball data file and replace IDs with full names\n",
    "updated_rows = []\n",
    "with open('2023_complete_batting_data.csv', 'r') as data_file: \n",
    "    data_reader = csv.reader(data_file)\n",
    "    header = next(data_reader)  \n",
    "    updated_rows.append(header)\n",
    "\n",
    "    # iterates through each row of the data\n",
    "    for row in data_reader:\n",
    "        \n",
    "        # converts the date format from 'YYYYMMDD' to 'MM/DD/YYYY'\n",
    "        date_str = row[11]\n",
    "        date_format = datetime.strptime(date_str, '%Y%m%d').strftime('%m/%d/%Y')\n",
    "        row[11] = date_format\n",
    "        \n",
    "        # replaces the player id with the player's full name\n",
    "        player_id = row[1] \n",
    "        if player_id in name_id_map:\n",
    "            row[1] = name_id_map[player_id] \n",
    "\n",
    "        # adds the updated row\n",
    "        updated_rows.append(row)\n",
    "\n",
    "# writes the updated csv file with all data and cleaned\n",
    "with open('2023_full_batting_stats_cleaned.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.writer(updated_file)\n",
    "    writer.writerows(updated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            gid                 id team  b_ab  b_h  b_d  b_t  b_hr  b_rbi  \\\n",
      "0  BOS202303300     Cedric Mullins  BAL     4    1    0    0     0      1   \n",
      "1  BOS202303300    Adley Rutschman  BAL     5    5    0    0     1      4   \n",
      "2  BOS202303300  Anthony Santander  BAL     6    2    1    0     0      0   \n",
      "3  BOS202303300   Ryan Mountcastle  BAL     4    1    1    0     0      1   \n",
      "4  BOS202303300   Gunnar Henderson  BAL     3    0    0    0     0      0   \n",
      "\n",
      "   b_w  ...  min_wall_height max_wall_height daynight attendance precip  \\\n",
      "0    2  ...              3.0              37      day    36049.0   none   \n",
      "1    1  ...              3.0              37      day    36049.0   none   \n",
      "2    0  ...              3.0              37      day    36049.0   none   \n",
      "3    2  ...              3.0              37      day    36049.0   none   \n",
      "4    2  ...              3.0              37      day    36049.0   none   \n",
      "\n",
      "     sky  temp  winddir windspeed  season_batting_avg  \n",
      "0  sunny  38.0     ltor      12.0               0.226  \n",
      "1  sunny  38.0     ltor      12.0               0.273  \n",
      "2  sunny  38.0     ltor      12.0               0.257  \n",
      "3  sunny  38.0     ltor      12.0               0.267  \n",
      "4  sunny  38.0     ltor      12.0               0.260  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "final_csv = '2023_full_batting_stats_cleaned.csv'\n",
    "\n",
    "data = pd.read_csv(final_csv, low_memory = False)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitching Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''filters for relevant player pitching data'''\n",
    "# load csv file\n",
    "data = pd.read_csv('2023pitching.csv')\n",
    "\n",
    "# filters for columns with relevant data\n",
    "columns_to_keep = ['gid', 'id', 'team', 'p_ipouts', 'p_seq', 'p_h', 'p_r', 'p_er', 'p_w','p_k', 'p_hbp', 'p_wp', 'date', 'wl']  \n",
    "\n",
    "# creates a new data frame\n",
    "data_filtered = data[columns_to_keep]\n",
    "\n",
    "# save new csv file\n",
    "data_filtered.to_csv('2023_filtered_pitching_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''merges pitcher game log data with ball park and game info data'''\n",
    "\n",
    "# load ball park data into a dictionary\n",
    "stadium_data = {}\n",
    "with open('2023_filtered_ballpark_data.csv', 'r') as stadium_file:  \n",
    "    stadium_reader = csv.DictReader(stadium_file)\n",
    "    for row in stadium_reader:\n",
    "        # extracts the team name column\n",
    "        team_name = row['team_name'] \n",
    "        # uses team name as a key and the row as the value \n",
    "        stadium_data[team_name] = row  \n",
    "\n",
    "# loads game info data into a dictionary        \n",
    "gameinfo_data = {}\n",
    "with open('2023_filtered_gameinfo_data.csv', 'r') as gameinfo_file:  \n",
    "    gameinfo_reader = csv.DictReader(gameinfo_file)\n",
    "    for row in gameinfo_reader:\n",
    "        # extracts the 'gid' column\n",
    "        gid = row['gid']  \n",
    "        # uses the game id as a key and the row as the value\n",
    "        gameinfo_data[gid] = row  \n",
    "        \n",
    "\n",
    "# read pitching data, merge with ball park data, store updated rows\n",
    "updated_rows = []\n",
    "with open('2023_filtered_pitching_data.csv', 'r') as game_log_file: \n",
    "    pitching_log = csv.reader(game_log_file)\n",
    "    # captures the header row\n",
    "    header = next(pitching_log)  \n",
    "\n",
    "\n",
    "    # extarcts column names of ball park data\n",
    "    stadium_columns = list(stadium_data.values())[0].keys()  \n",
    "    # extracts column names of the game info data, excluding 'gid' since it already exist in the dataset\n",
    "    gameinfo_columns = list(gameinfo_data.values())[0].keys()\n",
    "    new_gameinfo_columns = []\n",
    "    \n",
    "    for col in gameinfo_columns:\n",
    "        if col != 'gid':\n",
    "            new_gameinfo_columns.append(col)\n",
    "\n",
    "    # appends the original header with additional stadium and game info columns           \n",
    "    updated_rows.append(header + list(stadium_columns) + new_gameinfo_columns)\n",
    "\n",
    "\n",
    "    # interates through each row of the pitching log data\n",
    "    for row in pitching_log:\n",
    "        # extracts the game id\n",
    "        gid = row[0]  \n",
    "        # extracts the first three letter of the game id, which is the team id\n",
    "        team_id = gid[:3] \n",
    "\n",
    "        # checks if the team id exists in the staidum data \n",
    "        if team_id in stadium_data:\n",
    "            \n",
    "            # retrieves he ball park data for that team and appends it to the row\n",
    "            # this can be done because the first three character of the game id represent the team if of the home team\n",
    "            stadium_info = stadium_data[team_id]\n",
    "            row.extend(stadium_info.values())\n",
    "        \n",
    "        # checks if the game id is in the game info data \n",
    "        if gid in gameinfo_data:\n",
    "            \n",
    "            # retrieves game info data to corresponding game id \n",
    "            gameinfo = gameinfo_data[gid]\n",
    "            updated_gameinfo = []\n",
    "            # removes the duplicate game id column and appends the other values\n",
    "            for key, value in gameinfo.items():\n",
    "                if key != 'gid':\n",
    "                    updated_gameinfo.append(value)\n",
    "\n",
    "            # adds game info data to current row\n",
    "            row.extend(updated_gameinfo)\n",
    "\n",
    "        # adds new data to row   \n",
    "        updated_rows.append(row)\n",
    "\n",
    "\n",
    "# new csv file output with updated data\n",
    "with open('2023_merged_pitching_data.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.writer(updated_file)\n",
    "    writer.writerows(updated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Season ERA: 5.870\n"
     ]
    }
   ],
   "source": [
    "'''calculates the season ERA for each pitcher and merge the dataset'''\n",
    "import csv\n",
    "\n",
    "# Initialize dictionary to store season totals\n",
    "season_avg_data = {}\n",
    "\n",
    "# Process input file to calculate season ERA totals for each pitcher\n",
    "with open('2023_merged_pitching_data.csv', 'r') as stat_file:\n",
    "    stat_reader = csv.DictReader(stat_file)\n",
    "    header = stat_reader.fieldnames + ['season_era']\n",
    "\n",
    "    for row in stat_reader:\n",
    "        name = row['id']\n",
    "        outs = int(row['p_ipouts'])\n",
    "        earned_runs = int(row['p_er'])\n",
    "\n",
    "        # Update season totals for the pitcher\n",
    "        if name in season_avg_data:\n",
    "            season_avg_data[name]['total_innings'] += (outs / 3)\n",
    "            season_avg_data[name]['total_earned_runs'] += earned_runs\n",
    "        else:\n",
    "            season_avg_data[name] = {'total_innings': (outs / 3), 'total_earned_runs': earned_runs}\n",
    "\n",
    "# Prepare updated rows with calculated season ERA\n",
    "updated_rows = []\n",
    "with open('2023_merged_pitching_data.csv', 'r') as stat_file:\n",
    "    stat_reader = csv.DictReader(stat_file)\n",
    "    for row in stat_reader:\n",
    "        name = row['id']\n",
    "        innings = season_avg_data[name]['total_innings']\n",
    "        earned_runs = season_avg_data[name]['total_earned_runs']\n",
    "\n",
    "        # Calculate season ERA\n",
    "        season_era = 9 * (earned_runs / innings)\n",
    "        row['season_era'] = f\"{season_era:.3f}\"\n",
    "\n",
    "        updated_rows.append(row)\n",
    "\n",
    "# Write updated data to a new CSV file\n",
    "with open('2023_complete_pitching_data.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.DictWriter(updated_file, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(updated_rows)\n",
    "\n",
    "# Calculate overall average season ERA (NEW ADDITION)\n",
    "season_eras = [\n",
    "    9 * (stats['total_earned_runs'] / stats['total_innings'])\n",
    "    for stats in season_avg_data.values()\n",
    "    if stats['total_innings'] > 0  # Avoid division by zero\n",
    "]\n",
    "\n",
    "average_season_era = sum(season_eras) / len(season_eras) if season_eras else 0\n",
    "\n",
    "# Print the overall average season ERA (does not affect functionality)\n",
    "print(f\"Average Season ERA: {average_season_era:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: ['gid', 'id', 'team', 'p_ipouts', 'p_seq', 'p_h', 'p_r', 'p_er', 'p_w', 'p_k', 'p_hbp', 'p_wp', 'date', 'wl', 'team_name', 'ballpark', 'left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height', 'daynight', 'attendance', 'precip', 'sky', 'temp', 'winddir', 'windspeed', 'season_era']\n",
      "Date column detected at index 12 based on column name.\n",
      "Data cleaning complete. Updated dataset saved.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the first row (header) to see the column names\n",
    "with open('2023_complete_pitching_data.csv', 'r') as data_file:\n",
    "    data_reader = csv.reader(data_file)\n",
    "    header = next(data_reader)\n",
    "    print(\"Columns in the dataset:\", header)\n",
    "\n",
    "# Automatically detect date column by checking column names or data format\n",
    "date_column_index = None\n",
    "\n",
    "# Detect column with YYYYMMDD pattern or name containing \"date\"\n",
    "for i, column in enumerate(header):\n",
    "    if \"date\" in column.lower():  # Name-based detection\n",
    "        date_column_index = i\n",
    "        print(f\"Date column detected at index {date_column_index} based on column name.\")\n",
    "        break\n",
    "\n",
    "if date_column_index is None:\n",
    "    # Fallback to detecting by data format in the first row\n",
    "    for i, row in enumerate(data_reader):\n",
    "        for col_index, value in enumerate(row):\n",
    "            try:\n",
    "                if len(value) == 8:  # Typical YYYYMMDD format length\n",
    "                    datetime.strptime(value, '%Y%m%d')\n",
    "                    date_column_index = col_index\n",
    "                    print(f\"Date column detected at index {date_column_index} based on data format.\")\n",
    "                    break\n",
    "            except ValueError:\n",
    "                continue\n",
    "        if date_column_index is not None:\n",
    "            break\n",
    "\n",
    "# Update rows with automatic date detection\n",
    "name_id_map = {}\n",
    "with open('2023allplayers.csv', 'r') as name_file:  \n",
    "    name_reader = csv.DictReader(name_file)\n",
    "    for row in name_reader:\n",
    "        name_id_map[row['id']] = f\"{row['first']} {row['last']}\"\n",
    "\n",
    "updated_rows = []\n",
    "\n",
    "with open('2023_complete_pitching_data.csv', 'r') as data_file: \n",
    "    data_reader = csv.reader(data_file)\n",
    "    header = next(data_reader)\n",
    "    updated_rows.append(header)\n",
    "\n",
    "    for row in data_reader:\n",
    "        if date_column_index is not None:\n",
    "            # Convert detected date column format\n",
    "            try:\n",
    "                date_str = row[date_column_index]\n",
    "                date_format = datetime.strptime(date_str, '%Y%m%d').strftime('%m/%d/%Y')\n",
    "                row[date_column_index] = date_format\n",
    "            except ValueError:\n",
    "                pass  # Handle rows where the date might not conform\n",
    "\n",
    "        # Replace player ID with name\n",
    "        player_id = row[1]\n",
    "        if player_id in name_id_map:\n",
    "            row[1] = name_id_map[player_id]\n",
    "\n",
    "        updated_rows.append(row)\n",
    "\n",
    "# Write cleaned data\n",
    "with open('2023_full_pitching_stats_cleaned.csv', 'w', newline='') as updated_file:\n",
    "    writer = csv.writer(updated_file)\n",
    "    writer.writerows(updated_rows)\n",
    "\n",
    "print(\"Data cleaning complete. Updated dataset saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            gid               id team  p_ipouts  p_seq  p_h  p_r  p_er  p_w  \\\n",
      "0  BOS202303300      Kyle Gibson  BAL        15      1    6    4     4    1   \n",
      "1  BOS202303300      Keegan Akin  BAL         3      2    1    0     0    0   \n",
      "2  BOS202303300     Cionel Perez  BAL         3      3    0    0     0    0   \n",
      "3  BOS202303300      Bryan Baker  BAL         2      4    2    3     3    1   \n",
      "4  BOS202303300  Logan Gillaspie  BAL         1      5    0    0     0    0   \n",
      "\n",
      "   p_k  ...  min_wall_height  max_wall_height daynight attendance precip  \\\n",
      "0    3  ...              3.0               37      day    36049.0   none   \n",
      "1    2  ...              3.0               37      day    36049.0   none   \n",
      "2    0  ...              3.0               37      day    36049.0   none   \n",
      "3    1  ...              3.0               37      day    36049.0   none   \n",
      "4    1  ...              3.0               37      day    36049.0   none   \n",
      "\n",
      "     sky  temp winddir  windspeed season_era  \n",
      "0  sunny  38.0    ltor       12.0      4.708  \n",
      "1  sunny  38.0    ltor       12.0      6.845  \n",
      "2  sunny  38.0    ltor       12.0      3.436  \n",
      "3  sunny  38.0    ltor       12.0      4.169  \n",
      "4  sunny  38.0    ltor       12.0      6.000  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "final_csv = '2023_full_pitching_stats_cleaned.csv'\n",
    "\n",
    "data = pd.read_csv(final_csv)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-input processing\n",
    "\n",
    "Prior to using the cleaned data, we need to process the data into a more readable format. This would mean fully separating into inputs and outputs as well as converting any categorical variables into binaries. \n",
    "\n",
    "To do so, we use pd.get_dummies to \"one-hot-encode\" our categorical variables to not place too much importance on any given point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21042 entries, 0 to 21061\n",
      "Data columns (total 28 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   left_field       21042 non-null  float64\n",
      " 1   center_field     21042 non-null  float64\n",
      " 2   right_field      21042 non-null  float64\n",
      " 3   min_wall_height  21042 non-null  float64\n",
      " 4   max_wall_height  21042 non-null  float64\n",
      " 5   attendance       21042 non-null  float64\n",
      " 6   temp             21042 non-null  float64\n",
      " 7   windspeed        21042 non-null  float64\n",
      " 8   season_era       21042 non-null  float64\n",
      " 9   daynight_day     21042 non-null  int32  \n",
      " 10  daynight_night   21042 non-null  int32  \n",
      " 11  precip_drizzle   21042 non-null  int32  \n",
      " 12  precip_none      21042 non-null  int32  \n",
      " 13  precip_rain      21042 non-null  int32  \n",
      " 14  precip_snow      21042 non-null  int32  \n",
      " 15  sky_cloudy       21042 non-null  int32  \n",
      " 16  sky_dome         21042 non-null  int32  \n",
      " 17  sky_overcast     21042 non-null  int32  \n",
      " 18  sky_sunny        21042 non-null  int32  \n",
      " 19  winddir_fromcf   21042 non-null  int32  \n",
      " 20  winddir_fromlf   21042 non-null  int32  \n",
      " 21  winddir_fromrf   21042 non-null  int32  \n",
      " 22  winddir_ltor     21042 non-null  int32  \n",
      " 23  winddir_rtol     21042 non-null  int32  \n",
      " 24  winddir_tocf     21042 non-null  int32  \n",
      " 25  winddir_tolf     21042 non-null  int32  \n",
      " 26  winddir_torf     21042 non-null  int32  \n",
      " 27  winddir_unknown  21042 non-null  int32  \n",
      "dtypes: float64(9), int32(19)\n",
      "memory usage: 3.1 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21042 entries, 0 to 21061\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   p_ipouts  21042 non-null  int64\n",
      " 1   p_h       21042 non-null  int64\n",
      " 2   p_r       21042 non-null  int64\n",
      " 3   p_er      21042 non-null  int64\n",
      " 4   p_w       21042 non-null  int64\n",
      " 5   p_k       21042 non-null  int64\n",
      " 6   p_hbp     21042 non-null  int64\n",
      " 7   p_wp      21042 non-null  int64\n",
      "dtypes: int64(8)\n",
      "memory usage: 1.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load pitching data\n",
    "pitching_data = pd.read_csv('2023_full_pitching_stats_cleaned.csv')\n",
    "\n",
    "# Define categorical data and input/output columns\n",
    "categorical_data = ['daynight', 'precip', 'sky', 'winddir']\n",
    "\n",
    "pitching_inputs = [\n",
    "    'left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height',\n",
    "    'attendance', 'temp', 'windspeed', 'season_era', 'daynight_day', 'daynight_night',\n",
    "    'precip_drizzle', 'precip_none', 'precip_rain', 'precip_snow', 'sky_cloudy', 'sky_dome',\n",
    "    'sky_overcast', 'sky_sunny', 'winddir_fromcf', 'winddir_fromlf', 'winddir_fromrf',\n",
    "    'winddir_ltor', 'winddir_rtol', 'winddir_tocf', 'winddir_tolf', 'winddir_torf', 'winddir_unknown'\n",
    "]\n",
    "pitching_outputs = ['p_ipouts', 'p_h', 'p_r', 'p_er', 'p_w', 'p_k','p_hbp', 'p_wp']\n",
    "\n",
    "# Handle missing values and categorical data\n",
    "pitching_data['precip'] = pitching_data['precip'].fillna('none')\n",
    "pitching_data = pitching_data.dropna()\n",
    "pitching_data = pd.get_dummies(pitching_data, columns=categorical_data)\n",
    "\n",
    "# Post-encoding dummy variables\n",
    "p_encoded_variables = [\n",
    "    'daynight_day', 'daynight_night', 'precip_drizzle', 'precip_none', 'precip_rain',\n",
    "    'precip_snow', 'sky_cloudy', 'sky_dome', 'sky_overcast', 'sky_sunny',\n",
    "    'winddir_fromcf', 'winddir_fromlf', 'winddir_fromrf', 'winddir_ltor', 'winddir_rtol',\n",
    "    'winddir_tocf', 'winddir_tolf', 'winddir_torf', 'winddir_unknown'\n",
    "]\n",
    "\n",
    "# Convert non-dummy columns to float\n",
    "non_encoded_columns = [\n",
    "    'left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height',\n",
    "    'attendance', 'temp', 'windspeed', 'season_era'\n",
    "]\n",
    "pitching_data[non_encoded_columns] = pitching_data[non_encoded_columns].astype(float)\n",
    "\n",
    "# Convert dummy-encoded columns to float\n",
    "pitching_data[p_encoded_variables] = pitching_data[p_encoded_variables].astype(int)\n",
    "\n",
    "# Extract input and output data\n",
    "pitching_input_data = pitching_data[pitching_inputs]\n",
    "pitching_output_data = pitching_data[pitching_outputs]\n",
    "\n",
    "# Verify data types\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(pitching_input_data.info())\n",
    "print(pitching_output_data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_ipouts</th>\n",
       "      <th>p_h</th>\n",
       "      <th>p_r</th>\n",
       "      <th>p_er</th>\n",
       "      <th>p_w</th>\n",
       "      <th>p_k</th>\n",
       "      <th>p_hbp</th>\n",
       "      <th>p_wp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_ipouts  p_h  p_r  p_er  p_w  p_k  p_hbp  p_wp\n",
       "0        15    6    4     4    1    3      1     0\n",
       "1         3    1    0     0    0    2      0     0\n",
       "2         3    0    0     0    0    0      0     0\n",
       "3         2    2    3     3    1    1      1     0\n",
       "4         1    0    0     0    0    1      0     0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitching_input_data.head()\n",
    "pitching_output_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49080 entries, 0 to 49118\n",
      "Data columns (total 28 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   left_field          49080 non-null  float64\n",
      " 1   center_field        49080 non-null  float64\n",
      " 2   right_field         49080 non-null  float64\n",
      " 3   min_wall_height     49080 non-null  float64\n",
      " 4   max_wall_height     49080 non-null  float64\n",
      " 5   attendance          49080 non-null  float64\n",
      " 6   temp                49080 non-null  float64\n",
      " 7   windspeed           49080 non-null  float64\n",
      " 8   season_batting_avg  49080 non-null  float64\n",
      " 9   daynight_day        49080 non-null  int32  \n",
      " 10  daynight_night      49080 non-null  int32  \n",
      " 11  precip_drizzle      49080 non-null  int32  \n",
      " 12  precip_none         49080 non-null  int32  \n",
      " 13  precip_rain         49080 non-null  int32  \n",
      " 14  precip_snow         49080 non-null  int32  \n",
      " 15  sky_cloudy          49080 non-null  int32  \n",
      " 16  sky_dome            49080 non-null  int32  \n",
      " 17  sky_overcast        49080 non-null  int32  \n",
      " 18  sky_sunny           49080 non-null  int32  \n",
      " 19  winddir_fromcf      49080 non-null  int32  \n",
      " 20  winddir_fromlf      49080 non-null  int32  \n",
      " 21  winddir_fromrf      49080 non-null  int32  \n",
      " 22  winddir_ltor        49080 non-null  int32  \n",
      " 23  winddir_rtol        49080 non-null  int32  \n",
      " 24  winddir_tocf        49080 non-null  int32  \n",
      " 25  winddir_tolf        49080 non-null  int32  \n",
      " 26  winddir_torf        49080 non-null  int32  \n",
      " 27  winddir_unknown     49080 non-null  int32  \n",
      "dtypes: float64(9), int32(19)\n",
      "memory usage: 7.3 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 49080 entries, 0 to 49118\n",
      "Data columns (total 8 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   b_ab    49080 non-null  int64\n",
      " 1   b_h     49080 non-null  int64\n",
      " 2   b_d     49080 non-null  int64\n",
      " 3   b_t     49080 non-null  int64\n",
      " 4   b_hr    49080 non-null  int64\n",
      " 5   b_rbi   49080 non-null  int64\n",
      " 6   b_w     49080 non-null  int64\n",
      " 7   b_k     49080 non-null  int64\n",
      "dtypes: int64(8)\n",
      "memory usage: 3.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "batting_data = pd.read_csv('2023_complete_batting_data.csv', low_memory=False)\n",
    "\n",
    "# Define input and output columns\n",
    "batting_inputs = [\n",
    "    'left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height',\n",
    "    'attendance', 'temp', 'windspeed', 'season_batting_avg', 'daynight_day', 'daynight_night',\n",
    "    'precip_drizzle', 'precip_none', 'precip_rain', 'precip_snow', 'sky_cloudy', 'sky_dome',\n",
    "    'sky_overcast', 'sky_sunny', 'winddir_fromcf', 'winddir_fromlf', 'winddir_fromrf',\n",
    "    'winddir_ltor', 'winddir_rtol', 'winddir_tocf', 'winddir_tolf', 'winddir_torf', 'winddir_unknown'\n",
    "]\n",
    "batting_outputs = ['b_ab', 'b_h', 'b_d', 'b_t', 'b_hr', 'b_rbi', 'b_w', 'b_k']\n",
    "\n",
    "# Handle missing and categorical data\n",
    "batting_data['precip'] = batting_data['precip'].fillna('none')\n",
    "batting_data = batting_data.dropna()\n",
    "categorical_data = ['daynight', 'precip', 'sky', 'winddir']\n",
    "batting_data = pd.get_dummies(batting_data, columns=categorical_data)\n",
    "\n",
    "# Post-encoding dummy variables\n",
    "b_encoded_variables = [\n",
    "    'daynight_day', 'daynight_night', 'precip_drizzle', 'precip_none', 'precip_rain',\n",
    "    'precip_snow', 'sky_cloudy', 'sky_dome', 'sky_overcast', 'sky_sunny',\n",
    "    'winddir_fromcf', 'winddir_fromlf', 'winddir_fromrf', 'winddir_ltor', 'winddir_rtol',\n",
    "    'winddir_tocf', 'winddir_tolf', 'winddir_torf', 'winddir_unknown'\n",
    "]\n",
    "\n",
    "# Convert non-dummy columns to float\n",
    "non_encoded_columns = [\n",
    "    'left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height',\n",
    "    'attendance', 'temp', 'windspeed', 'season_batting_avg'\n",
    "]\n",
    "batting_data[non_encoded_columns] = batting_data[non_encoded_columns].astype(float)\n",
    "\n",
    "# Convert dummy-encoded columns to float\n",
    "batting_data[b_encoded_variables] = batting_data[b_encoded_variables].astype(int)\n",
    "\n",
    "# Extract input and output data\n",
    "batting_input_data = batting_data[batting_inputs]\n",
    "batting_output_data = batting_data[batting_outputs]\n",
    "\n",
    "# Verify data types\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(batting_input_data.info())\n",
    "print(batting_output_data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_field</th>\n",
       "      <th>center_field</th>\n",
       "      <th>right_field</th>\n",
       "      <th>min_wall_height</th>\n",
       "      <th>max_wall_height</th>\n",
       "      <th>attendance</th>\n",
       "      <th>temp</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>season_era</th>\n",
       "      <th>daynight_day</th>\n",
       "      <th>daynight_night</th>\n",
       "      <th>precip_drizzle</th>\n",
       "      <th>precip_none</th>\n",
       "      <th>precip_rain</th>\n",
       "      <th>precip_snow</th>\n",
       "      <th>sky_cloudy</th>\n",
       "      <th>sky_dome</th>\n",
       "      <th>sky_overcast</th>\n",
       "      <th>sky_sunny</th>\n",
       "      <th>winddir_fromcf</th>\n",
       "      <th>winddir_fromlf</th>\n",
       "      <th>winddir_fromrf</th>\n",
       "      <th>winddir_ltor</th>\n",
       "      <th>winddir_rtol</th>\n",
       "      <th>winddir_tocf</th>\n",
       "      <th>winddir_tolf</th>\n",
       "      <th>winddir_torf</th>\n",
       "      <th>winddir_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>310.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>36049.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.708</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>310.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>36049.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.845</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>310.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>36049.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.436</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>310.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>36049.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.169</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>36049.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   left_field  center_field  right_field  min_wall_height  max_wall_height  \\\n",
       "0       310.0         420.0        302.0              3.0             37.0   \n",
       "1       310.0         420.0        302.0              3.0             37.0   \n",
       "2       310.0         420.0        302.0              3.0             37.0   \n",
       "3       310.0         420.0        302.0              3.0             37.0   \n",
       "4       310.0         420.0        302.0              3.0             37.0   \n",
       "\n",
       "   attendance  temp  windspeed  season_era  daynight_day  daynight_night  \\\n",
       "0     36049.0  38.0       12.0       4.708             1               0   \n",
       "1     36049.0  38.0       12.0       6.845             1               0   \n",
       "2     36049.0  38.0       12.0       3.436             1               0   \n",
       "3     36049.0  38.0       12.0       4.169             1               0   \n",
       "4     36049.0  38.0       12.0       6.000             1               0   \n",
       "\n",
       "   precip_drizzle  precip_none  precip_rain  precip_snow  sky_cloudy  \\\n",
       "0               0            1            0            0           0   \n",
       "1               0            1            0            0           0   \n",
       "2               0            1            0            0           0   \n",
       "3               0            1            0            0           0   \n",
       "4               0            1            0            0           0   \n",
       "\n",
       "   sky_dome  sky_overcast  sky_sunny  winddir_fromcf  winddir_fromlf  \\\n",
       "0         0             0          1               0               0   \n",
       "1         0             0          1               0               0   \n",
       "2         0             0          1               0               0   \n",
       "3         0             0          1               0               0   \n",
       "4         0             0          1               0               0   \n",
       "\n",
       "   winddir_fromrf  winddir_ltor  winddir_rtol  winddir_tocf  winddir_tolf  \\\n",
       "0               0             1             0             0             0   \n",
       "1               0             1             0             0             0   \n",
       "2               0             1             0             0             0   \n",
       "3               0             1             0             0             0   \n",
       "4               0             1             0             0             0   \n",
       "\n",
       "   winddir_torf  winddir_unknown  \n",
       "0             0                0  \n",
       "1             0                0  \n",
       "2             0                0  \n",
       "3             0                0  \n",
       "4             0                0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitching_input_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For predictions, we are also going to use average valuesand modes. For numerical data we will use the mean. For categorical datas, we will use the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Tune or Neural Network (NN), we are using different numbers. To do so, we will use the gridsearch CV function to process our Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements for Neural Networks\n",
    "import sklearn as sk\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support our multi-output classifier, we needed to create a multioutput evaluation method to score our tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multioutput_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy for multi-output targets.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    accuracies = [\n",
    "        accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "        for i in range(y_true.shape[1])\n",
    "    ]\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "\n",
    "\n",
    "multioutput_scorer = make_scorer(multioutput_accuracy, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitching Model\n",
    "Firstly we will start with our pitching neural network. We start off with scaling our data and reducing our number of dimensions. From there, we will run it through our MLPClassifier Algorithm from Sci-kit learn. We will determine what hyperparameters work best for our neural network by using the GridSearchCV function to get a cross validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(pline, param_grid, cv\u001b[38;5;241m=\u001b[39mkf, scoring\u001b[38;5;241m=\u001b[39mmultioutput_scorer, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Cross-validate using the subsampled data\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m pitching_nested_score \u001b[38;5;241m=\u001b[39m cross_val_score(gs, gs_pitching_input_data\u001b[38;5;241m.\u001b[39mvalues, gs_pitching_output_data\u001b[38;5;241m.\u001b[39mvalues, \n\u001b[0;32m     26\u001b[0m                                         cv\u001b[38;5;241m=\u001b[39mkf,scoring\u001b[38;5;241m=\u001b[39mmultioutput_scorer, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNested cross-validation scores:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pitching_nested_score)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, pitching_nested_score\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:719\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    717\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 719\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    720\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    721\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    722\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    723\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    724\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    725\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    726\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    727\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    728\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m    729\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    730\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    731\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    732\u001b[0m )\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:430\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 430\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    431\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    432\u001b[0m         clone(estimator),\n\u001b[0;32m    433\u001b[0m         X,\n\u001b[0;32m    434\u001b[0m         y,\n\u001b[0;32m    435\u001b[0m         scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[0;32m    436\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    437\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    438\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    439\u001b[0m         parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    440\u001b[0m         fit_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[0;32m    441\u001b[0m         score_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mscore,\n\u001b[0;32m    442\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    443\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    444\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    445\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    446\u001b[0m     )\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    448\u001b[0m )\n\u001b[0;32m    450\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create Pipeline of processes to run through\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "pline = Pipeline([('scaling', sk.preprocessing.StandardScaler()), ('pca', PCA()),\n",
    "                  ('nnet', MultiOutputClassifier(MLPClassifier( early_stopping= True)))])\n",
    "\n",
    "# Defines Parameters to Test\n",
    "param_grid = {\n",
    "    'pca__n_components':[5, 7, 9],\n",
    "    'nnet__estimator__hidden_layer_sizes':[10,20,30],\n",
    "    'nnet__estimator__activation': ['relu'],\n",
    "    'nnet__estimator__alpha':[0.0001,0.01],\n",
    "    'nnet__estimator__max_iter':[500, 1000]\n",
    "}\n",
    "\n",
    "# Subsample the data for grid search (randomly selecting 10,000 samples)\n",
    "gs_pitching_input_data = pitching_input_data.sample(n=10000, random_state=42)\n",
    "gs_pitching_output_data = pitching_output_data.loc[gs_pitching_input_data.index]\n",
    "\n",
    "\n",
    "# Grid Search + Scoring\n",
    "gs = GridSearchCV(pline, param_grid, cv=kf, scoring=multioutput_scorer, n_jobs=-1)\n",
    "\n",
    "# Cross-validate using the subsampled data\n",
    "pitching_nested_score = cross_val_score(gs, gs_pitching_input_data.values, gs_pitching_output_data.values, \n",
    "                                        cv=kf,scoring=multioutput_scorer, n_jobs=-1)\n",
    "\n",
    "print(\"Nested cross-validation scores:\", pitching_nested_score)\n",
    "print(\"Mean Accuracy: \", pitching_nested_score.mean() * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing a grid search, we will now extract the best values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Best Parameters\n",
    "gs.fit(pitching_input_data, pitching_output_data)\n",
    "best_params = gs.best_params_\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "#{'nnet__estimator__activation': 'relu', 'nnet__estimator__alpha': 0.0001, 'nnet__estimator__hidden_layer_sizes': 30, 'pca__n_components': 5} From previous try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Final Algorithm\n",
    "\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(\n",
    "    pitching_input_data.values,  # Ensure NumPy arrays\n",
    "    pitching_output_data.values, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Generate Final Algorithm with the best parameters\n",
    "final_model = Pipeline([\n",
    "    ('scaling', sk.preprocessing.StandardScaler()), \n",
    "    ('pca', PCA(n_components=best_params['pca__n_components'])),\n",
    "    ('nnet', MultiOutputClassifier(MLPClassifier(\n",
    "        activation=best_params['nnet__estimator__activation'],\n",
    "        hidden_layer_sizes=best_params['nnet__estimator__hidden_layer_sizes'],\n",
    "        alpha=best_params['nnet__estimator__alpha'],\n",
    "        max_iter=best_params['nnet__estimator__max_iter'],\n",
    "        early_stopping=True\n",
    "    )))\n",
    "])\n",
    "\n",
    "# Train the final model on the training set\n",
    "final_model.fit(X_train, y_train)\n",
    "print(\"Final model training completed.\")\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "# Compute and print the accuracy\n",
    "test_accuracy = multioutput_accuracy(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate detailed classification reports for each output variable\n",
    "for i, col in enumerate(pitching_output_data.columns):\n",
    "    print(f\"Classification Report for {col}:\")\n",
    "    print(sk.metrics.classification_report(y_test[:, i], y_pred[:, i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pitching_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "print(\"Final model saved as 'pitching_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pitching_input_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the model, we will now evaluate how much of an impact each of the input values has on the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "class MultiOutputPipelineWrapper:\n",
    "    def __init__(self, pipeline, target_index):\n",
    "        self.pipeline = pipeline\n",
    "        self.target_index = target_index\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict only for the specified output\n",
    "        return self.pipeline.predict(X)[:, self.target_index]\n",
    "\n",
    "# Wrap your pipeline for a specific target (e.g., `p_ipouts`)\n",
    "target_index = 0  # Specify the index of the output you want to analyze\n",
    "wrapped_model = MultiOutputPipelineWrapper(final_model, target_index)\n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.Explainer(wrapped_model.predict, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Plot SHAP summary for the analyzed output\n",
    "shap.summary_plot(shap_values, X_test, feature_names=pitching_input_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batting Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline of processes to run through\n",
    "pline2 = Pipeline([('scaling', sk.preprocessing.StandardScaler()), ('pca', PCA()),\n",
    "                   ('nnet', MultiOutputClassifier(MLPClassifier(max_iter=1000, early_stopping=True)))])\n",
    "\n",
    "# Defines Parameters to Test\n",
    "param_grid2 = {\n",
    "    'pca__n_components':[5, 7, 9],\n",
    "    'nnet__estimator__hidden_layer_sizes':[10,20,30],\n",
    "    'nnet__estimator__activation': ['relu'],\n",
    "    'nnet__estimator__alpha':[0.0001,0.01],\n",
    "    'nnet__estimator__max_iter':[500, 1000]\n",
    "}\n",
    "\n",
    "# Subsample the data for grid search\n",
    "gs_batting_input_data = batting_input_data.sample(10000, random_state=42)\n",
    "gs_batting_output_data = batting_output_data.sample(10000, random_state=42)\n",
    "\n",
    "# Grid Search + Scoring\n",
    "gs2 = GridSearchCV(pline2, param_grid2, cv=5, scoring=multioutput_scorer, n_jobs=-1)\n",
    "\n",
    "# Cross-validate using the subsampled data\n",
    "batting_nested_score = cross_val_score(gs2, \n",
    "                                       gs_batting_input_data.values, \n",
    "                                       gs_batting_output_data.values, \n",
    "                                       cv=3, \n",
    "                                       scoring=multioutput_scorer, \n",
    "                                       n_jobs=-1)\n",
    "\n",
    "print(\"Nested cross-validation scores:\", batting_nested_score)\n",
    "print(\"Mean Accuracy: \", batting_nested_score.mean() * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Best Parameters\n",
    "gs2.fit(batting_input_data, batting_output_data)\n",
    "best_params2 = gs2.best_params_\n",
    "print(best_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train2, X_test2, y_train2, y_test2 = sk.model_selection.train_test_split(\n",
    "    batting_input_data.values,  # Ensure NumPy arrays\n",
    "    batting_output_data.values, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Generate Final Algorithm with the best parameters\n",
    "final_model2 = Pipeline([\n",
    "    ('scaling', sk.preprocessing.StandardScaler()), \n",
    "    ('pca', PCA(n_components=best_params2['pca__n_components'])),\n",
    "    ('nnet', MultiOutputClassifier(MLPClassifier(\n",
    "        activation=best_params2['nnet__estimator__activation'],\n",
    "        hidden_layer_sizes=best_params2['nnet__estimator__hidden_layer_sizes'],\n",
    "        alpha=best_params2['nnet__estimator__alpha'],\n",
    "        max_iter=1000,\n",
    "        early_stopping=True\n",
    "    )))\n",
    "])\n",
    "\n",
    "# Train the final model on the training set\n",
    "final_model2.fit(X_train2, y_train2)\n",
    "print(\"Final model training completed.\")\n",
    "\n",
    "# Test the model and compute predictions\n",
    "y_pred2 = final_model2.predict(X_test2)\n",
    "\n",
    "# Compute and print the accuracy\n",
    "test_accuracy2 = multioutput_accuracy(y_test2, y_pred2)\n",
    "print(f\"Test Accuracy: {test_accuracy2 * 100:.2f}%\")\n",
    "\n",
    "# Generate detailed classification reports for each output variable\n",
    "for i, col in enumerate(batting_output_data.columns):\n",
    "    print(f\"Classification Report for {col}:\")\n",
    "    print(sk.metrics.classification_report(y_test2[:, i], y_pred2[:, i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batting_model = pickle.load(open('batting_model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "class MultiOutputPipelineWrapper:\n",
    "    def __init__(self, pipeline, target_index):\n",
    "        self.pipeline = pipeline\n",
    "        self.target_index = target_index\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict only for the specified output\n",
    "        return self.pipeline.predict(X)[:, self.target_index]\n",
    "\n",
    "# Wrap your pipeline for a specific target (e.g., `p_ipouts`)\n",
    "target_index = 0  # Specify the index of the output you want to analyze\n",
    "wrapped_model = MultiOutputPipelineWrapper(batting_model, target_index)\n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.Explainer(wrapped_model.predict, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Plot SHAP summary for the analyzed output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, feature_names=pitching_input_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"batting_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_model2, f)\n",
    "print(\"Final model saved as 'batting_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Model\n",
    "\n",
    "After tuning and training the model, we will now use the model. You can load in the model by using the Pickle load method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitching_model = pickle.load(open('pitching_model.pkl', 'rb'))\n",
    "print(\"Pitching model imported from 'pitching_model.pkl' as pitching_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the Player Class\n",
    "\n",
    "We created the base player class to have the player object contain all the non-player based classification data and then have it separate with subclasses as pitchers and batters. We included default values for environmental and player stats.\n",
    "\n",
    "For batters, we have an average batting average of 0.226\n",
    "\n",
    "For pitchers, we have an average season ERA of 5.870"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For environmental variables, we used either the mean of numerical values or used the mode of categorical variables. \n",
    "This provided us with stadium values of the following:\n",
    "\n",
    "Average Column Values:  \n",
    "left_field         331.833333  \n",
    "center_field       404.166667  \n",
    "right_field        328.333333  \n",
    "min_wall_height      7.553333  \n",
    "max_wall_height     14.266667  \n",
    "  \n",
    "We also got game variables of the following:  \n",
    "\n",
    "Average Values for Numeric Columns:  \n",
    "attendance    29356.347087  \n",
    "temp             72.413835  \n",
    "windspeed         6.466828  \n",
    "  \n",
    "Most Frequent Values for Categorical Columns:  \n",
    "daynight      night  \n",
    "precip         none  \n",
    "sky          cloudy  \n",
    "winddir     unknown  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    left_field, center_field, right_field, min_wall_height, max_wall_height, attendance, temp, windspeed = (-1,) * 8\n",
    "    daynight, precip, winddir = ('',)*3\n",
    "    def __init__(self, name = 'unknown', lf = 331.833, cf = 404.167, rf = 328.333, min_wh = 7.553, max_wh = 14.267, att = 29356,\n",
    "                 t = 74.41, ws = 6.466, dn = 'night', pp = 'none', s = 'cloudy', wd = 'unknown'):\n",
    "        self.name = name\n",
    "        self.left_field = lf\n",
    "        self.center_field = cf\n",
    "        self.right_field = rf\n",
    "        self.min_wall_height= min_wh\n",
    "        self.max_wall_height = max_wh, \n",
    "        self.attendance = att\n",
    "        self.temp = t \n",
    "        self.windspeed = ws\n",
    "        self.daynight = dn \n",
    "        self.precip = pp\n",
    "        self.winddir = wd\n",
    "        self.stat = -1\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "\n",
    "\n",
    "    def set_environment_data(self, ballpark_name):\n",
    "        ballparks = pd.read_csv('ballparks.csv')\n",
    "        name_column = 'ballpark'\n",
    "        environment_columns = ['left_field', 'center_field', 'right_field', 'min_wall_height', 'max_wall_height']\n",
    "\n",
    "        # Filter the DataFrame for the specified ballpark\n",
    "        filtered_row = ballparks[ballparks[name_column] == ballpark_name]\n",
    "\n",
    "        if filtered_row.empty:\n",
    "            print( f\"Ballpark '{ballpark_name}' not found.\")\n",
    "            pass\n",
    "\n",
    "        # Extract the relevant environment columns and set them as instance attributes\n",
    "        environment_data = filtered_row.iloc[0]\n",
    "        self.left_field = environment_data['left_field']\n",
    "        self.center_field = environment_data['center_field']\n",
    "        self.right_field = environment_data['right_field']\n",
    "        self.min_wall_height = environment_data['min_wall_height']\n",
    "        self.max_wall_height = environment_data['max_wall_height']\n",
    "\n",
    "    #one-hot-encode helper method\n",
    "    def one_hot_encode(self, value, categories):\n",
    "        return [1 if value == category else 0 for category in categories]\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Base numeric features\n",
    "        features = [\n",
    "            self.left_field, \n",
    "            self.center_field, \n",
    "            self.right_field, \n",
    "            self.min_wall_height, \n",
    "            self.max_wall_height, \n",
    "            self.attendance, \n",
    "            self.temp, \n",
    "            self.windspeed,\n",
    "            self.stat #for batter or pitcher\n",
    "        ]\n",
    "\n",
    "        # One-hot encode categorical variables\n",
    "        features += self.one_hot_encode(self.daynight, ['day', 'night'])\n",
    "        features += self.one_hot_encode(self.precip, ['drizzle', 'none', 'rain', 'snow'])\n",
    "        features += self.one_hot_encode(self.sky, [\n",
    "            'sky_cloudy', 'sky_dome', 'sky_overcast', 'sky_sunny'\n",
    "        ])\n",
    "        features += self.one_hot_encode(self.winddir, [\n",
    "            'fromcf', 'fromlf', 'fromrf', 'ltor', 'rtol', 'tocf', 'tolf', 'torf', 'unknown'\n",
    "        ])\n",
    "\n",
    "        return iter(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pitcher(Player):\n",
    "    def __init__(self, era=5.870, **kwargs):\n",
    "        super().__init__(**kwargs)  # Pass all parent arguments to Player\n",
    "        self.stat = era  # Set ERA as the stat\n",
    "\n",
    "\n",
    "class Batter(Player):\n",
    "    def __init__(self, batting_avg=0.226, **kwargs):\n",
    "        super().__init__(**kwargs)  # Pass all parent arguments to Player\n",
    "        self.stat = batting_avg  # Set batting average as the stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = Pitcher()\n",
    "pred =pitching_model.predict(np.array(list(bob)).reshape(1,-1))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get players season average (era if pitcher, batting avg if batter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = input(\"Enter a player's name: \").lower()\n",
    "\n",
    "def get_season_avg(name):\n",
    "\n",
    "    pitching_file = '2023_full_pitching_stats_cleaned.csv'\n",
    "    batting_file = '2023_full_batting_stats_cleaned.csv'\n",
    "\n",
    "    pitching_data = pd.read_csv(pitching_file)\n",
    "    batting_data = pd.read_csv(batting_file)\n",
    "\n",
    "    name_column = 'id'\n",
    "    batting_avg_column = 'season_batting_avg'\n",
    "    era_avg_column = 'season_era'\n",
    "\n",
    "    batting_data[name_column] = batting_data[name_column].str.lower()\n",
    "    pitching_data[name_column] = pitching_data[name_column].str.lower()\n",
    "    \n",
    "    if name in batting_data[name_column].values and name in pitching_data[name_column].values:\n",
    "        batter_avg = batting_data[batting_data[name_column] == name][batting_avg_column].values[0]\n",
    "        era_avg = pitching_data[pitching_data[name_column] == name][era_avg_column].values[0]\n",
    "        spec = input(\"Pitcher or Batter: (p or b)\").lower()\n",
    "        if spec == 'p':\n",
    "            return Pitcher(era = float(era_avg))\n",
    "        else:\n",
    "            return Batter(batting_avg = float(batter_avg))\n",
    "\n",
    "    elif name in batting_data[name_column].values:\n",
    "        batter_avg = batting_data[batting_data[name_column] == name][batting_avg_column].values[0]\n",
    "        return Batter(batting_avg = float(batter_avg))\n",
    "\n",
    "    elif name in pitching_data[name_column].values:\n",
    "        era_avg = pitching_data[pitching_data[name_column] == name][era_avg_column].values[0]\n",
    "        return Pitcher(era = float(era_avg))\n",
    "    \n",
    "    else:\n",
    "        spec = input(\"Pitcher or Batter: (p or b)\").lower()\n",
    "        if spec == 'p':\n",
    "            return Pitcher()\n",
    "        else:\n",
    "            return Batter()\n",
    "        \n",
    "\n",
    "jose = get_season_avg(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pitcher(Player):\n",
    "    def __init__(self, era=3.5, **kwargs):\n",
    "        super().__init__(**kwargs)  # Pass all parent arguments to Player\n",
    "        self.stat = era  # Set ERA as the stat\n",
    "\n",
    "    def Readout(self):\n",
    "        \n",
    "        p1 = pitching_model.predict(list(self))\n",
    "\n",
    "        p1pitchedouts= p1[0]\n",
    "        p1hitsallowed = p1[1]\n",
    "        p1runsgivneup = p1[2]\n",
    "        p1earnedrunsagaisnt = p1[3]\n",
    "        p1causedwalks = p1[4]\n",
    "        p1thrownstrikes = p1[5]\n",
    "        p1hitbypitch = p1[6]\n",
    "        p1wildpitch = p1[7]\n",
    "        \n",
    "        #average time in game for first player\n",
    "        \n",
    "        p1inningstopitch = p1pitchedouts/3\n",
    "        \n",
    "        # strikeouts per walk\n",
    "        p1soutper9 = p1[5]/p1[4]\n",
    "        \n",
    "        # WHIP (walks+hit  / innigs pithced)\n",
    "        p1walktostrike = (p1[4] + p1[1]) / p1inningstopitch\n",
    "        \n",
    "        sigstring = 'nice'\n",
    "        \n",
    "        #rain = self.precip\n",
    "        #if rain  == 'rain':\n",
    "         #   rainstring = 'rainy'\n",
    "        \n",
    "        wind = self.windspd\n",
    "        if wind > 15:\n",
    "            windstring = 'windy'           \n",
    "        \n",
    "        sky = self.sky\n",
    "        \n",
    "        if sky == 'overcast':\n",
    "          skystring = 'cloudy'\n",
    "        \n",
    "        if rainstring == 'rainy':\n",
    "            sigstring = rainstring\n",
    "            \n",
    "        elif windstring == 'windy':\n",
    "              sigstring = windstring\n",
    "            \n",
    "        elif skystring == 'overcast':\n",
    "                  sigstring = skystring\n",
    "        \n",
    "        #tuning placements of information on comparsion visual\n",
    "        p1onebasecoor = (.558,-.568)\n",
    "        p1twobasecoor = (-.081,-.429)\n",
    "        p1threebasecoor = (-.620,-.568)\n",
    "        p1fourbasecoor = (-.081,-.800)\n",
    "        \n",
    "        \n",
    "        flattitle = f\"Expected perfomance of {p1.name} at {location} on a {sigstring}  {p1.daynight}\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class Batter(Player):\n",
    "    def __init__(self, batting_avg=0.275, **kwargs):\n",
    "        super().__init__(**kwargs)  # Pass all parent arguments to Player\n",
    "        self.stat = batting_avg  # Set batting average as the stat\n",
    "\n",
    "    def Readout(self):\n",
    "        p1 = batting_model.predict(list(self))\n",
    "        \n",
    "\n",
    "        p1onebaseA = p1[1] - p1[2] - p1[3]\n",
    "        p1twobaseA = p1[2]\n",
    "        p1threebaseA = p1[3]\n",
    "        p1fourbaseA = p1[4]\n",
    "        \n",
    "        #batting average for first player\n",
    "        p1battavg = p1[1] / p1[0]\n",
    "        \n",
    "        # strikeout percentage for first player\n",
    "        p1soutper = p1[7]/p1[0]\n",
    "        \n",
    "        # walk to strikeout ratio for first player\n",
    "        p1walktostrike = p1[6] /p1[7]\n",
    "           \n",
    "        \n",
    "        sigstring = 'nice'\n",
    "        \n",
    "        skystring = p1.sky\n",
    "\n",
    "        wind = p1.windspd\n",
    "        if wind > 8:\n",
    "            windstring = 'windy'\n",
    "        \n",
    "        #rain = p1.precip\n",
    "        #if rain > 1 inch:\n",
    "         #   rainstring = 'rainy'\n",
    "        \n",
    "        if rainstring == 'rainy':\n",
    "            sigstring = rainstring\n",
    "            \n",
    "        elif windstring == 'windy':\n",
    "              sigstring = windstring\n",
    "            \n",
    "        else:\n",
    "                  sigstring = skystring\n",
    "         # tuning placements of information on comparsion visual\n",
    "        p1onebasecoor = (.558,-.568)\n",
    "        p1twobasecoor = (-.081,-.429)\n",
    "        p1threebasecoor = (-.620,-.568)\n",
    "        p1fourbasecoor = (-.081,-.800)       \n",
    "\n",
    "        flattitle = f\"Expected perfomance of p1.name at {location} on a {sigstring} {p1.daynight}\"\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "        #Display the baseball diamond image\n",
    "          \n",
    "        ax.imshow(img, aspect='auto', extent=[-1, 1, -1, 1])\n",
    "          \n",
    "        \n",
    "        ax.annotate(p1onebaseA, xy = p1onebasecoor ,  xytext = p1onebasecoor) \n",
    "        \n",
    "        ax.annotate(p1twobaseA, xy = p1twobasecoor , xytext = p1twobasecoor ) \n",
    "        \n",
    "        ax.annotate(p1threebaseA, xy = p1threebasecoor , xytext = p1threebasecoor ) \n",
    "        \n",
    "        ax.annotate(p1fourbaseA, xy = p1fourbasecoor,  xytext = p1fourbasecoor ) \n",
    "        \n",
    "        \n",
    "        #scoreboard readout\n",
    "        boardcoord = -.2,.4\n",
    "        annotations = [f\"Batting Average: {p1battavg}\", f\"Runs Batted In: {p1[5]}\", f\"Strikeout Percentage: {p1soutper}\",f\"Walk to Strike Ratio: {p1walktostrike}\"] \n",
    "        \n",
    "        y_positions = np.linspace(.4, .185, len(annotations)) \n",
    "        \n",
    "        #Evenly spaced y positions \n",
    "        for y_pos, text in zip(y_positions, annotations): \n",
    "            ax.annotate(text, xy = (boardcoord[0], y_pos), xytext=(boardcoord[0], y_pos), fontsize = 6)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "        def Compare(player2):\n",
    "            p1 = batting_model.predict(list(self))\n",
    "    \n",
    "            p1onebaseA = p1[1] - p1[2] - p1[3]\n",
    "            p1twobaseA = p1[2]\n",
    "            p1threebaseA = p1[3]\n",
    "            p1fourbaseA = p1[3]\n",
    "            \n",
    "            #batting average for first player\n",
    "            p1battavg = p1[1] / p1[0]\n",
    "            \n",
    "            # strikeout percentage for first player\n",
    "            p1soutper = p1[7]/p1[0]\n",
    "            \n",
    "            # walk to strikeout ratio for first player\n",
    "            p1walktostrike = p1[6] /p1[7]\n",
    "            \n",
    "            \n",
    "            \n",
    "            #for comparsion to follow\n",
    "            pathannotA = (p1onebaseA, p1twobaseA,p1threebaseA,p1fourbaseA)\n",
    "            \n",
    "            # player2.battingoutput() = some list with the same indexes as above\n",
    "            \n",
    "            p2 = batting_model.predict(list(player2))\n",
    "       \n",
    "            p2onebaseA = p2[1] - p2[2] - p2[3]\n",
    "            p2twobaseA = p2[2]\n",
    "            p2threebaseA = p2[3]\n",
    "            p2fourbaseA = p2[3]\n",
    "            \n",
    "            #batting average for first player\n",
    "            p2battavg = p2[1] / p2[0]\n",
    "            \n",
    "            # strikeout percentage for first player\n",
    "            p2soutper = p2[7]/p2[0]\n",
    "            \n",
    "            # walk to strikeout ratio for first player\n",
    "            p2walktostrike = p2[6] /p2[7]\n",
    "            \n",
    "            \n",
    "            \n",
    "            pathannotB = (p2onebaseB,p2twobaseB,p2threebaseB,p2fourbaseB)\n",
    "            \n",
    "            \n",
    "                \n",
    "            comparray = zip(pathannotA, pathannotB)\n",
    "            \n",
    "            # color of annotation dict\n",
    "            basedict = {\n",
    "                        1: {\"p1onebaseA\": [], \"p2onebaseB\": []},\n",
    "                    2: {\"p1twobaseA\": [], \"p2twobaseB\": []},\n",
    "                    3: {\"p1threebaseA\": [], \"p2threebaseB\": []},\n",
    "                4: {\"p1fourbaseA\": [], \"p2fourbaseB\": []}\n",
    "            }\n",
    "            \n",
    "            vicitercount = 1\n",
    "            for pair in comparray:\n",
    "                if pair[0] > pair[1]:\n",
    "                    basedict[vicitercount] = {pair[0]: 'green', pair[1]: 'red'}\n",
    "                \n",
    "                elif pair[0] < pair[1]:\n",
    "                    basedict[vicitercount] = {pair[0]: 'red', pair[1]: 'green'}\n",
    "                \n",
    "                elif pair[0] == pair[1]:\n",
    "                    # Both values are equal, assign both as black\n",
    "                    basedict[vicitercount] = {pair[0]: 'black', pair[1]: 'black'}\n",
    "                \n",
    "            vicitercount += 1\n",
    "         \n",
    "            #Pull conditons\n",
    "                \n",
    "            sigstring = 'nice'\n",
    "            \n",
    "            skystring = p1.sky\n",
    "            \n",
    "            \n",
    "            wind = p1.windspd\n",
    "            if wind > 8:\n",
    "                windstring = 'windy'\n",
    "                \n",
    "            #rain = p1.precip\n",
    "            #if rain > inch:\n",
    "              #  rainstring = 'rainy'\n",
    "    \n",
    "            if rainstring == 'rainy':\n",
    "                sigstring = rainstring\n",
    "                \n",
    "            elif windstring == 'windy':\n",
    "                  sigstring = windstring\n",
    "                \n",
    "            else:\n",
    "                      sigstring = skystring\n",
    "                    \n",
    "            # tuning placements of information on comparsion visual\n",
    "            p1onebasecoor = (.558,-.568)\n",
    "            p1twobasecoor = (-.081,-.429)\n",
    "            p1threebasecoor = (-.620,-.568)\n",
    "            p1fourbasecoor = (-.081,-.800)\n",
    "                   \n",
    "            p2onebasecoor = (.600,-.568)\n",
    "            p2twobasecoor = (-.066,-.429)\n",
    "            p2threebasecoor = (-.520,-.568)\n",
    "            p2fourbasecoor = (-.066,-.800)   \n",
    "            \n",
    "            comptitle = f\"Expected performance of p1.name and player2.name at {location} on a {sigstring} {p1.daynight}\"\n",
    "           \n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "            \n",
    "              # Display the baseball diamond image\n",
    "            ax.imshow(img, aspect='auto', extent=[-1, 1, -1, 1])\n",
    "            \n",
    "          \n",
    "            # each base requires two annotations\n",
    "            # First \n",
    "            \n",
    "            ax.annotate(p1onebaseA , xy =(p1onebasecoor) , xytext =  p1onebasecoor, color= basedict[1][p1onebaseA])\n",
    "            \n",
    "            ax.annotate(p2onebaseB , xy =(p2onebasecoor) , xytext =  p2onebasecoor, color=basedict[1][p2twobaseB])\n",
    "              \n",
    "            # Second\n",
    "    \n",
    "            ax.annotate(p1twobaseA , xy =(p1twobasecoor) , xytext =  p1twobasecoor, color=basedict[2][p1onebaseA])\n",
    "            \n",
    "            ax.annotate(p2twobaseB , xy =(p2twobasecoor) ,  xytext =  p2twobasecoor, color=basedict[2][p2twobaseB])\n",
    "            \n",
    "            # Third\n",
    "            \n",
    "            ax.annotate(p1threebaseA , xy =(p1threebasecoor) , xytext =  p1threebasecoor, color=basedict[3][p1onebaseA])\n",
    "            \n",
    "            ax.annotate(p2threebaseB , xy =(p2threebasecoor) , xytext =  p2threebasecoor, color=basedict[3][p2twobaseB])\n",
    "            \n",
    "            # Home\n",
    "            \n",
    "            ax.annotate(p1fourbaseA , xy =(p1fourbasecoor) , xytext =  p1fourbasecoor, color=basedict[4][p1onebaseA])\n",
    "            \n",
    "            ax.annotate(p2fourbaseB , xy =(p2fourbasecoor) , xytext =  p2fourbasecoor, color=basedict[4][p2twobaseB])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Get players season average (era if pitcher, batting avg if batter)\n",
    "name = input(\"Enter a player's name: \").lower()\n",
    "\n",
    "def get_season_avg(name):\n",
    "\n",
    "    pitching_file = '2023_full_pitching_stats_cleaned.csv'\n",
    "    batting_file = '2023_full_batting_stats_cleaned.csv'\n",
    "\n",
    "    pitching_data = pd.read_csv(pitching_file)\n",
    "    batting_data = pd.read_csv(batting_file)\n",
    "\n",
    "    name_column = 'id'\n",
    "    batting_avg_column = 'season_batting_avg'\n",
    "    era_avg_column = 'season_era'\n",
    "\n",
    "    batting_data[name_column] = batting_data[name_column].str.lower()\n",
    "    pitching_data[name_column] = pitching_data[name_column].str.lower()\n",
    "    \n",
    "    if name in batting_data[name_column].values and name in pitching_data[name_column].values:\n",
    "        batter_avg = batting_data[batting_data[name_column] == name][batting_avg_column].values[0]\n",
    "        era_avg = pitching_data[pitching_data[name_column] == name][era_avg_column].values[0]\n",
    "        spec = input(\"Pitcher or Batter: (p or b)\").lower()\n",
    "        if spec == 'p':\n",
    "            return Pitcher(era = float(era_avg))\n",
    "        else:\n",
    "            return Batter(batting_avg = float(batter_avg))\n",
    "\n",
    "    elif name in batting_data[name_column].values:\n",
    "        batter_avg = batting_data[batting_data[name_column] == name][batting_avg_column].values[0]\n",
    "        return Batter(batting_avg = float(batter_avg))\n",
    "\n",
    "    elif name in pitching_data[name_column].values:\n",
    "        era_avg = pitching_data[pitching_data[name_column] == name][era_avg_column].values[0]\n",
    "        return Pitcher(era = float(era_avg))\n",
    "    \n",
    "    else:\n",
    "        spec = input(\"Pitcher or Batter: (p or b)\").lower()\n",
    "        if spec == 'p':\n",
    "            return Pitcher()\n",
    "        else:\n",
    "            return Batter()\n",
    "        \n",
    "\n",
    "jose = get_season_avg(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = Batter()\n",
    "print(list(jose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob_data = np.array(list(bob)).reshape(1,-1)\n",
    "pred_class = pitching_model.predict(bob_data)\n",
    "pred_prob = pitching_model.predict_proba(bob_data)\n",
    "# ['left_field', 'center_field', 'right_field','min_wall_height','max_wall_height',\n",
    "                                    # 'attendance','temp','windspeed','season_era', 'daynight_day', 'daynight_night', 'precip_drizzle', 'precip_none', 'precip_rain', \n",
    "                                    # 'precip_snow', 'sky_cloudy', 'sky_dome', 'sky_overcast', 'sky_sunny', 'winddir_fromcf', 'winddir_fromlf', 'winddir_fromrf', 'winddir_ltor', \n",
    "                                    # 'winddir_rtol', 'winddir_tocf', 'winddir_tolf', 'winddir_torf', 'winddir_unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_class)\n",
    "print(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = Batter(name=\"bob\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
